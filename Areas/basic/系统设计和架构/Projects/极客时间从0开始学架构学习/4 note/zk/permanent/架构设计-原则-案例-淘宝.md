---
tags:
  - permanent
---
## 1. 核心观点  
- 功能安全是一个逐步完善的过程，而且**往往都是在问题出现后才能有针对性的提出解决方案，我们永远无法预测系统下一个漏洞在哪里**，也不敢说自己的系统肯定没有任何问题。
- 互联网系统的架构安全目前并没有太好的设计手段来实现，更**多地是依靠运营商或者云服务商强大的带宽和流量清洗的能力，较少自己来设计和实现**。

## 2. 背景/出处  

- https://mp.weixin.qq.com/s/t5u1OrDWXp3Yeq4y89_M3g
- https://book.douban.com/subject/30335935/

## 3. 展开说明  
### 1.0
2003年5月 - 2004年1月

**当时对整个项目组来说压力最大的就是时间，怎么在最短的时间内把一个从来就没有的网站从零开始建立起来**？了解淘宝历史的人知道淘宝是在 2003 年 5 月10 日上线的，这之间只有一个月。要是你在这个团队里，你怎么做？我们的答案就是：买一个来。

**淘宝当时在初创时，没有过多考虑技术是否优越、性能是否海量以及稳定性如何，主要的考虑因素就是：快**！因为此时业务要求快速上线，时间不等人，等你花几个月甚至十几个月搞出一个强大的系统出来，可能市场机会就没有了，黄花菜都凉了。

同样，在考虑如何买的时候，淘宝的决策依据主要也是“快”​。
买一个网站显然比做一个网站要省事一些，但是他们的梦想可不是做一个小网站而已，要做大，就不是随便买个就行的，要有比较低的维护成本，要能够方便地扩展和二次开发。那接下来就是第二个问题：买一个什么样的网站？答案是：轻量一点的，简单一点的。

**买一个系统是为了“快速可用”​，而买一个轻量级的系统是为了“快速开发”**。因为系统上线后肯定有大量的需求需要做，这时能够快速开发就非常重要。

从这个实例我们可以看到：淘宝最开始的时候业务要求就是“快”​，因此反过来要求技术同样要“快”​，业务决定技术，这里架构设计和选择主要遵循的是“合适原则”和“简单原则”​。第一代的技术架构如图所示。


LAMP 架构的网站：Linux + Apache + MySQL + PHP。淘宝早期系统架构如下图所示。其中，pear DB  是一个PHP模块，复杂数据访问层。
![](asserts/Pasted%20image%2020251204144603.png)

### 1.1
2004年1月 - 2004年5月

淘宝网推出后，由于正好碰到“非典”​，网购很火爆，加上采取了成功的市场运作，流量和交易量迅速上涨，业务发展很快，在2003年底，MySQL已经撑不住了。

- 由于数据量增大，**Mysql 逐渐替换为 Oracle**。因为 Oracle 容量大、稳定、安全、性能高，还有人才储备。
- NAS
	- 后来数据量变大了，本地存储不行了。买了NAS（Network Attached Storage，网络附属存储）​，NetApp的NAS存储作为了数据库的存储设备，加上Oracle RAC（Real Application Clusters，实时应用集群）来实现负载均衡。
- PHP 语言需要解决数据库链接问题，因此架构中引入了 SQL Realy 组件，这是一个开源的可以提供连接池代理服务的组件。
	- 但对于PHP语言来说，它是放在Apache上的，每一个请求都会对数据库产生一个连接，它没有连接池这种功能（Java语言有Servlet容器，可以存放连接池）​。那如何是好呢？这帮人打探到eBay在PHP下面用了一个连接池的工具，是BEA卖给他们的。我们知道BEA的东西都很贵，我们买不起，于是多隆在网上寻寻觅觅，找到一个开源的连接池代理服务SQL Relay。

![](asserts/Pasted%20image%2020251204144647.png)


### 2.0 版本
2004年2月 - 2005年3月

淘宝切换到Java的原因很有趣，主要因为找了一个PHP的开源连接池SQL Relay连接到Oracle，而这个代理经常死锁，死锁了就必须重启，而数据库又必须用Oracle，于是决定换个开发语言。最后淘宝挑选了Java，而且当时挑选Java，也是请Sun公司的人，这帮人很历害，先是将淘宝网站从PHP热切换到了Java，后来又做了支付宝

这次切换的最主要原因是因为**技术影响了业务的发展，频繁的死锁和重启对用户业务产生了严重的影响，从业务的角度来看这是不得不解决的技术问题。**

为了解决数据库链接的问题，更好地使用Oracle数据库，需要进一步切换编程语言。同时，Java是当时最成熟的开发框架，人才较多，后续维护成本也很低。整体的架构开始切换为以Java语言为主。其中MVC框架是阿里的WebX，控制层用了 EJB，未来环境查询压力，引入了搜索引擎。

![](asserts/Pasted%20image%2020251204144704.png)

### 2.1版本
2004年10月 - 2007年1月

在业务发展的过程中，进行了数据分库、放弃EJB、引入Spring、加入缓存、加入CDN等工作，形成了进一步优化的架构。，其实都是围绕着提高容量、提高性能、节约成本来做的。

我们思考一下，为什么在前面的阶段，淘宝考虑的都是“快”​，而现在开始考虑“容量、性能、成本”了呢？而且为什么这个时候不采取“买”的方式来解决容量、性能、成本问题呢？


简单来说，就是“买”也搞不定了，此时的业务发展情况是这样的：随着数据量的继续增长，到了2005年，商品数有1663万，PV有8931万，注册会员有1390万，这给数据和存储带来的压力依然很大，数据量大，性能就慢。**原有的方案存在固有缺陷，随着业务的发展，已经不是靠“买”就能够解决问题了，此时必须从整个架构上去进行调整和优化**。比如说Oracle再强大，在做like类搜索的时候，也不可能做到纯粹的搜索系统如Solr、Sphinx等的性能，因为这是机制决定的。 

另外，随着规模的增大，纯粹靠买的一个典型问题开始成为重要的考虑因素，那就是成本。当买一台两台Oracle的时候，可能对成本并不怎么关心，但如果要买100台Oracle，成本就是一个关键因素了。这就是“量变带来质变”的一个典型案例，业务和系统发生质变后，架构设计遵循“演化原则”的思想，需要再一次重构甚至重写。



![](asserts/Pasted%20image%2020251204144724.png)


### 3.0
- TFS之后版本
我个人认为是淘宝技术飞跃的开始，简单来说就是淘宝技术从商用转为“自研”​，典型的就是去IOE化。分布式时代我认为是淘宝技术的修炼成功，到了这个阶段，自研技术已经自成一派，除了支撑本身的海量业务，也开始影响整个互联网的技术发展。

去IOE化指企业在关键业务系统中**逐步减少或替换 IBM、Oracle、EMC 等传统高成本基础设施产品**，转而采用自主可控、成本更低、可扩展性更强的技术体系。其核心目标是：

- 摆脱对封闭商业系统的技术与采购依赖
    
- 建立开放、可演进、可扩展的 IT 架构
    
- 降低成本、提升灵活性、增强对业务变化的响应能力
    

IOE 各对应：

- **I：IBM 小型机（POWER 架构服务器）**
    
- **O：Oracle 数据库**
    
- **E：EMC 存储系统**

**为解决大量小文件存储问题和查询性能问题，创造了 TFS（Taobao File System）和 Tair（分布式缓存/KV 系统）**。这是典型的**去IOE化架构实践**。

![](asserts/Pasted%20image%2020251204144738.png)


- 2.2.6 系统拆分
到 2008 年初，整个主站系统（有了机票、彩票系统之后，把原来的系统叫做主站）的容量已经到了瓶颈，商品数在一个亿以上，PV在2.5亿个以上，会员超过了5000万个。这时 Oracle 的连接池数量都不够用了，数据库的容量到了极限，即使上层系统加机器也无法继续扩容，**只有把底层的基础服务继续拆分，从底层开始扩容，上层才能扩展，才能容纳以后三五年的增长**。
  

整个项目名称为“五彩石”（女娲炼石补天用的石头），这是继2004年从LAMP架构到Java架构之后的第二次脱胎换骨。


基础业务服务包括：

- UIC：用户信息中心（User Information Center）
    
- Forest：类目属性服务 （森林，与类目属性有点神似）
    

  

其中核心业务系统的名词如下，这些中心级别的服务**只提供原子级的业务逻辑**，如根据ID查找商品、创建交易、减少库存等操作：

- TC：交易中心（Trade Center）
    
- IC：商品中心（Item Center）
    
- SC：店铺中心（Shop Center）
    

  

再往上一层是业务系统：

- TM：交易业务（Trade Manager）
    
- IM：商品业务（Item Manager）
    
- SM：店铺业务（Shop Manager，后来改名叫 SS， 即 Shop System）
    
- Detail：商品详情

![](asserts/Pasted%20image%2020251204144759.png)


系统拆分后，系统交互就变得比较复杂了，具体如下图所示。这进一步推动了事实调用中间件 HSF、异步消息中间件 Notify、多系统会话中间件 Session 等中间件的发展。
![](asserts/Pasted%20image%2020251204144809.png)


### **中间件发展**



#### 淘宝文件系统-TFS
|        |                                                   |                                                                            |
| ------ | ------------------------------------------------- | -------------------------------------------------------------------------- |
| 时间     | 问题背景                                              | 解决方案                                                                       |
| 2007年前 | 淘宝有很多文件要存储，如图片、商品描述、交易快照，这些文件读取需要频繁寻道和换道，读取延迟延时较高 | 采用商用存储系统，应用 NetApp 公司的文件存储系统                                               |
| 2007年  | 淘宝网的图片文件数量以每年3倍的速度增长，NetAPP公司的文件存储系统难以满足淘宝发展      | 借鉴GFS（Google File System）的设计论文，开发出了淘宝文件存储系统（TaoBao File System, TFS）的1.0版本 |
| 2009年  | 持续优化，如：改善心跳和同步的性能，对元数据存储和清理磁盘空间也做了优化              | 形成了 TFS 1.3版本                                                              |
#### 淘宝KV 缓存系统-Tair
|       |                                                           |                                                                                 |
| ----- | --------------------------------------------------------- | ------------------------------------------------------------------------------- |
| 时间    | 问题背景                                                      | 解决方案                                                                            |
| 2004年 | 需要解决页面端静态片段的缓存                                            | 使用了 ESI（Edge Side Includes）的缓存（Cache）                                           |
|       | 需要解决一些动态数据的缓存，如浏览量，是否可以从内存里面取？                            | 提供了一个基于 Berkeley DB 的缓存系统，叫 TBstore。                                            |
| 2007年 | 独立了用户中心 UIC（User Information Center），用户访问 UIC 量级很高，需要使用缓存 | 为 UIC 写了缓存系统 TDBM，TDBM 抛弃了 Berkeley DB 的持久功能，数据全部存放在内存中。                        |
| 2009年 | 内存利用率和吞吐量上可以进一步提升                                         | 参考了 memcached 的内存结构，改进了 TDBM 的集群分布方式，推出了 TDBM 2.0。                              |
|       | TDBM、TBstore 的数据接口和用途都很相似                                 | 开发团队把二者合并，推出了淘宝自创的 Key-value 缓存系统 -- Tair（TaoBao Pair 的意思，Pair 即 Key-Value 数据对） |
#### 高性能服务框架 HSF

|         |                                                                                      |                                                                               |
| ------- | ------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------- |
| 时间      | 问题背景                                                                                 | 解决方案                                                                          |
| 2008年左右 | 原来同步服务很多，有Jar包模式、Hessian接口、WebService、Socket、甚至HTTP请求等。系统拆分之后，系统越来越多，系统之间需要提供统一的同步服务 | 建设统一的同步服务 HSF（High-Speed Service Framework）。HSF 是一个分布式的标准 Service 方式的 RPC 框架。 |

 
#### 消息中间件 Notify
|         |                                               |                                                     |
| ------- | --------------------------------------------- | --------------------------------------------------- |
| 时间      | 问题背景                                          | 解决方案                                                |
| 2005年   | 支付宝和银行之间交互，有时需要通过异步通知进行，但是消息很难保证不丢失、也很容易重复通知。 | 使用 MQ（Message Queue）的方式来解决这个问题，并通过存放数据库来保证消费需求      |
| 2008年左右 | 系统拆分之后，系统间也需要异步通知。消息量级整体又增加了一个层次，需要更强大的消息中间件。 | 建设消息中间件 Notify。Notify 是一个分布式的消息中间件系统，支持消息的订阅、发送和消费。 |

#### 分布式数据访问层 TDDL
|         |                                            |                                                                          |
| ------- | ------------------------------------------ | ------------------------------------------------------------------------ |
| 时间      | 问题背景                                       | 解决方案                                                                     |
| 早期      | 商品库切分的时候，就需要提供统一访问能力                       | 基于 ibatis 进行很浅的封装，形成 CommonDAO 的组件                                       |
| 2009年左右 | Oracle 成本较高，需要对一部分数据使用 MySQL，需要对外提供统一数据访问。 | 建设 TDDL 1.0 版本（Taobao Distributed Data layer）,有人取外号“头都大了”                |
|         | 随着TDDL的业务越来越多，TDDL开始进入“评价”、“商品”等核心业务。      | 完善异构数据源增量复制、解决容量压力测试等问题，形成TDDL 2.0 时代。                                   |
|         | 进入“交易”后，一些功能受到业务挑战“不需要的功能为什么要放到流程里”？       | 产生了TDDL 3.0 版本：**将代码逻辑进行了节分，将单机主备切换、数据源管理独立出来。同时开始做工具，数据库运维平台的组件被提了出来**。 |
| 2013年左右 | 建设更多的数据工具来提效                               | 进入工具平台时代：建设了“愚公”数据迁移平台；以内部开源方式提出了“精卫”数据增量复制平台。                           |
#### Seesion 框架
|     |                                                               |                                                                                  |
| --- | ------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| 时间  | 问题背景                                                          | 解决方案                                                                             |
|     | 客户端与服务端交互需要保持会话。Http协议本身是无状态的，需要通过 Session 来解决服务端和浏览器保持状态的述求。 | 早期使用 Cookie                                                                      |
|     | Cookie 带来了流量传输成本，网络流量成本越来越高。                                  | 采用了集中式的缓存区Session。发展为 Tbsession，解决 Session 的这些问题：1）客户端存储；2）服务端存储；3）配置统一管理；4）动态更新 |
### **开放平台**

2006年底，阿里巴巴提出了 Work at Alibaba 的战略，要为中小企业提供一个工作平台。工作平台需要是一个开放的平台，因为卖家需要是长尾的。因此，需要做一个支持二次开发的工作平台，半开放式地满足卖家的长尾管理需求。

|   |   |   |
|---|---|---|
|时间|主题|主要内容|
|2006年|起点|开始做原型|
|2007年|萌芽|SOA 盛行的年代，内部架构服务化成为开放的第一步，如：淘宝的HSF（OSGI）。|
|2008年|雏形|平台开放淘宝服务30个，主要解决三个问题：1）服务路由；2）服务接口标准化；3）授权。|
|2009年|产品化|平台开放服务100多个，卖家工具成为服务市场的主流。主要基于性能问题牵引，提供更好的技术方案。|
|2010年|平台化|开放服务300多个，SNS热潮的兴起带动了淘江湖的买家应用。API 的开放包装由开放平台交还给业务自身，平台提供更快的接入能力。同时需要考虑资源的使用隔离。|
|2011年|市场化|开放服务758个，营销工具崭露头角，淘宝客成为开放新宠。开始重点考虑开放的安全性。|
|2012年|垂直化|平台开放淘宝服务900多个，无线乘势而起。业务开始主动要开放，开放后开始运营服务，培育ISV市场。平台侧开始建设 JS SDK 和 无线 SDK（IOS，安卓）；同时启动“聚石塔”项目，提供弹性计算和存储能力及可靠的安全网络环境给 ISV。|
### 总结

我们现在很多日常的讨论可能还会出现在本文的问题列表中，可见“解决的策略”是可以不断被打磨完善的，同时方案也会受到市场的变化而反反复复，在曲折中前进。

在这个过程中，业务和技术是什么关系呢？ 从关键节点的角度看，**我感觉是两朵大浪在双向奔赴，在某个交织点进行碰撞形成融合，然后各自回落再次蓄能**。而这个势能则来源于重要的业务变化，背后是在满足市场需求的过程中，产生了累加的创造性的设计，形成了新的模式。

总的来说，历史已过，未来还在书写，如何吸收好这些前人策略，走好未来的路，还取决于大家的思考，路在大家的脚下。但是，隐约中，感觉有一股传承之流在流淌~

## 4. 与其他卡片的关联  
- 前置卡片：
	- [架构设计-复杂度来源：高可用、高性能、低成本、安全、规模-基本概念](架构设计-复杂度来源：高可用、高性能、低成本、安全、规模-基本概念.md)
- 后续卡片：
	- 
- 相似主题：

## 5. 应用/启发  
- 可以如何应用在工作、学习、生活中  
- 引发的思考与问题  

## 6. 待办/进一步探索  
- [ ] 深入阅读 xxx  
- [ ] 验证这个观点的边界条件  
