- 背景：内存大小不断增长导致文件系统性能**很大程度上取决于写入性能**
    - why：因为读取将在缓存中进行处理→文件系统性能很大程度上取决于写入性能
	    -  内存大小不断增长→内存中缓存更多数据→磁盘流量将越来越多地由写入组成
	- 随机I/O性能与顺序I/O性能之间存在巨大的差距，且不断扩大：
	    - 传输带宽每年增加约50%～100%。寻道和旋转延迟成本下降得较慢，可能每年5%～10%。因此，如果能够以顺序方式使用磁盘，则可以获得巨大的性能优势，随着时间的推移而增长。
	- 现有文件系统在许多常见工作负载上表现不佳。FFS [MJLF84]会执行大量写入，以创建大小为一个块的新文件：一个用于新的inode，一个用于更新inode位图...等
	    - 导致许多短寻道和随后的旋转延迟，因此性能远远低于峰值顺序带宽。
	- 传统文件系统不支持RAID。
- 结论：因此，**理想的文件系统会专注于写入性能，并尝试利用磁盘的顺序带宽。**
	- 此外，它在常见工作负载上表现良好，这种负载不仅写出数据，还经常更新磁盘上的元数据结构。最后，它可以在RAID和单个磁盘上运行良好。
- 新型文件系统**Rosenblum和Ousterhout称为LFS，是日志结构文件系统（Log-structured File System）的缩写。**
	- 写入磁盘时，LFS首先将所有更新（包括元数据！）缓冲在内存段中。当段已满时，它会在一次长时间的顺序传输中写入磁盘，并传输到磁盘的未使用部分。LFS永远不会覆写现有数据，而是始终将段写入空闲位置。由于段很大，因此可以有效地使用磁盘，并且文件系统的性能接近其峰值。
- 关键问题：随机I/O性能与顺序I/O性能之间存在巨大的差距，如何让读写变成顺序的？文件系统如何将所有写入转换为顺序写入？对于读取，此任务是不可能的，因为要读取的所需块可能是磁盘上的任何位置。但是，对于写入，文件系统总是有一个选择，而这正是我们希望利用的选择。
# 43.1 按顺序写入磁盘

**简单地将所有更新（例如数据块、inode等）顺序写入磁盘的这一基本思想是LFS的核心。**
当用户写入数据块时，不仅是数据被写入磁盘；还有其他需要更新的元数据（metadata）​。在这个例子中，让我们将文件的inode（I）也写入磁盘，并将其指向数据块D。
![](Pasted%20image%2020250525194936.png)
# 43.2　顺序而高效地写入

问题：遗憾的是，**（单单）顺序写入磁盘并不足以保证高效写入。**
	例如，假设我们在时间T向地址A写入一个块。然后等待一会儿，再向磁盘写入地址A+1（下一个块地址按顺序），但是在时间T+δ。遗憾的是，在第一次和第二次写入之间，磁盘已经旋转。当你发出第二次写入时，它将在提交之前等待一大圈旋转（具体地说，如果旋转需要时间Trotation，则磁盘将等待Trotation−δ，然后才能将第二次写入提交到磁盘表面）。因此，你可以希望看到简单地按顺序写入磁盘不足以实现最佳性能。实际上，你**必须向驱动器发出大量连续写入（或一次大写入）才能获得良好的写入性能。**
解决：引入缓冲技术
- 写入缓冲：多次IO合并成一次
	为了达到这个目的，LFS使用了一种称为**写入缓冲**[1]（write buffering）的古老技术。在写入磁盘之前，LFS会跟踪内存中的更新。收到足够数量的更新时，会立即将它们写入磁盘，从而确保有效使用磁盘。
- 实现
	- LFS一次写入的大块更新被称为段（segment）。虽然这个术语在计算机系统中被过度使用，但这里的意思是LFS用来对写入进行分组的大块。因此，在写入磁盘时，LFS会缓冲内存段中的更新，然后将该段一次性写入磁盘。只要段足够大，这些写入就会很有效。下面是一个例子，其中LFS将两组更新缓冲到一个小段中。实际段更大（几MB）。第一次更新是对文件j的4次块写入，第二次是添加到文件k的一个块。然后，LFS立即将整个七个块的段提交到磁盘。这些块的磁盘布局如下：
	- ![](Pasted%20image%2020250525195129.png)

# 43.3　要缓冲多少

取决于磁盘本身，特别是与传输速率相比定位开销有多高

例如，假设在每次写入之前定位（即旋转和寻道开销）大约需要Tposition s。进一步假设磁盘传输速率是Rpeak MB / s。在这样的磁盘上运行时，LFS在写入之前应该缓冲多少？每次写入时，都需要支付固定的定位成本。因此，为了摊销（amortize）这笔成本，你需要写入多少？写入越多就越好（显然）​，越接近达到峰值带宽。假设要写入D MB数据。写数据块的时间Twrite是定位时间Tposition的加上D的传输时间

Twrite =Tposion+(D/Rpeak)

因此，有效写入速率（Reffective）就是写入的数据量除以写入的总时间，即

Rreffective =D/Twrite =D/(Tposion+(D/Rpeak))
我们感兴趣的是，让有效速率（Reffective）接近峰值速率。
我们希望有效速率与峰值速率的比值是某个分数F，其中0 <F<1（典型的F可能是0.9，即峰值速率的90%）​。
Reffective = F×Rpeak

![](Pasted%20image%2020250525195913.png)
让有效速率达到 F×RpeakF 
解出 D
![](Pasted%20image%2020250525200003.png)
# 43.4　问题：查找inode

要了解如何在LFS中找到inode，让我们简单回顾一下如何在典型的UNIX文件系统中查找inode。在典型的文件系统（如FFS）甚至老UNIX文件系统中，查找inode很容易，因为它们以数组形式组织，并放在磁盘的固定位置上。例如，老UNIX文件系统将所有inode保存在磁盘的固定位置。因此，给定一个inode号和起始地址，要查找特定的inode，只需将inode号乘以inode的大小，然后将其加上磁盘数组的起始地址，即可计算其确切的磁盘地址。
给定一个inode号，基于数组的索引是快速而直接的。在FFS中查找给定inode号的inode仅稍微复杂一些，因为FFS将inode表拆分为块并在每个柱面组中放置一组inode。因此，必须知道每个inode块的大小和每个inode的起始地址。之后的计算类似，也很容易。
在LFS中，生活比较艰难。为什么？好吧，我们已经设法将inode分散在整个磁盘上！更糟糕的是，我们永远不会覆盖，因此最新版本的inode（即我们想要的那个）会不断移动。

# 43.5　通过间接解决方案：inode映射

在inode号和inode之间引入了一个间接层（level of indirection）。imap是一个结构，它将inode号作为输入，并生成最新版本的inode的磁盘地址。每次将inode写入磁盘时，imap都会使用其新位置进行更新。

遗憾的是，**imap需要保持持久（写入磁盘）。这样做允许LFS在崩溃时仍能记录inode位置**，从而按设想运行。因此有一个问题：imap应该驻留在磁盘上的哪个位置？

当然，它可以存在于磁盘的固定部分。遗憾的是，由于它经常更新，因此需要更新文件结构，然后写入imap，因此性能会受到影响（每次的更新和imap的固定位置之间，会有更多的磁盘寻道）。

与此不同，**LFS将inode映射的块放在它写入所有其他新信息的位置旁边。**
- 因此，当将数据块追加到文件k时，LFS实际上将新数据块，其inode和一段inode映射一起写入磁盘，如下所示：

在该图中，imap数组存储在标记为imap的块中，它告诉LFS，inode k位于磁盘地址A1。接下来，这个inode告诉LFS它的数据块

# 43.6　检查点区域

聪明的读者（就是你，对吗？）可能已经注意到了这里的问题。我们如何找到inode映射，现在它的各个部分现在也分布在整个磁盘上？归根到底：文件系统必须在磁盘上有一些固定且已知的位置，才能开始文件查找。

LFS在磁盘上只有这样一个固定的位置，称为检查点区域（checkpoint region，CR）。

- **检查点区域包含指向最新的inode映射片段的指针（即地址），因此可以通过首先读取CR来找到inode映射片段。**

请注意，检查点区域仅定期更新（例如每30s左右），因此性能不会受到影响。因此，磁盘布局的整体结构包含一个检查点区域（指向内部映射的最新部分），每个inode映射块包含inode的地址，inode指向文件（和目录），就像典型的UNIX文件系统一样。

下面的例子是检查点区域（注意它始终位于磁盘的开头，地址为0），以及单个imap块，inode和数据块。一个真正的文件系统当然会有一个更大的CR（事实上，它将有两个，我们稍后会理解），许多imap块，当然还有更多的inode、数据块等。

![](Pasted%20image%2020250525214556.png)
# 43.7　从磁盘读取文件：回顾

## LFS 读取文件的工作流程解析

### 1. 起点：检查点区域（Checkpoint Region）

- **定义与作用**
    
    检查点区域是 LFS 在磁盘上的特殊数据结构，保存了指向整个 **inode 映射（imap）** 的磁盘地址。
    
    它类似于传统文件系统中的超级块，主要**用于快速定位最新的文件元数据。**
    
- **工作原理**
    
    读取文件时，LFS 首先从磁盘读取检查点区域，获取当前最新的 **imap** 位置。
    
    然后将整个 **imap** 加载到内存缓存中。
    
- **为什么要缓存整个 imap？**
    
    因为 LFS 将所有 inode 的磁盘地址映射都集中管理在 imap 中，缓存它可以避免频繁磁盘查找，提高访问效率。
    

### 2. 通过 imap 定位 inode

- 当需要访问某个文件时，LFS 根据文件的 inode 号，在内存中的 imap 中查找该 inode 的磁盘地址。
- 读取 inode 最新版本，从而获得文件的元信息和数据块指针。

---

### 3. 读取文件数据块

- 在拿到 inode 后，LFS 访问数据块的方式与典型 UNIX 文件系统完全一致：
    - 通过直接指针读取数据块，
    - 通过单重、双重甚至三级间接指针间接访问更远的数据块。
- 这里的 I/O 数量和传统文件系统类似，因为数据访问的逻辑结构相同。

### 总结

- 读取文件
    - 读取检查点→inode映射在内存中缓存
    - node映射提供功能：iNode→数据
        - 在通常情况下，从磁盘读取文件时，LFS应执行与典型文件系统相同数量的I/O，整个imap被缓存，因此LFS在读取过程中所做的额外工作是**在imap中查找inode的地址。**

# 43.8　目录如何

幸运的是，目录结构与传统的UNIX文件系统基本相同，

inode映射还解决了LFS中递归更新问题

- 递归更新问题
    - 更新inode时，它在磁盘上的位置都会发生变化。
    - 它在磁盘上的位置都会发生变化。如果我们不小心，这也会导致对指向该文件的目录的更新，然后必须更改该目录的父目录，依此类推，一路沿文件系统树向上。 为什么

# 43.9　一个新问题：垃圾收集

你可能已经注意到LFS的另一个问题；它会反复将最新版本的文件（包括其inode和数据）写入磁盘上的新位置。此过程在保持写入效率的同时，意味着LFS会在整个磁盘中分散旧版本的文件结构。我们（毫不客气地）将这些旧版本称为**垃圾（garbage）。**

- 覆盖导致整个数据块没用。
    
    例如，假设有一个由inode号k引用的现有文件，该文件指向单个数据块D0。我们现在覆盖该块，生成新的inode和新的数据块。由此产生的LFS磁盘布局看起来像这样（注意，简单起见，我们省略了imap和其他结构。还需要将一个新的imap大块写入磁盘，以指向新的inode）：
    
    - 在图中，可以看到inode和数据块在磁盘上有两个版本，一个是旧的（左边那个），一个是当前的，因此是活的（live，右边那个）。对于覆盖数据块的简单行为，LFS必须持久许多新结构，从而在磁盘上留下上述块的旧版本。
- 添加导致旧iNode没用
    
    另外举个例子，假设我们将一块添加到该原始文件k中。在这种情况下，会生成新版本的inode，但旧数据块仍由旧inode指向。因此，它仍然存在，并且与当前文件系统分离：
    
    
- 那么，应该**如何处理这些旧版本的inode、数据块**等呢？
    
    > 保留形成版本控制文件系统
    > 
    > 保留那些旧版本并允许用户恢复旧文件版本（例如，当他们意外覆盖或删除文件时，这样做可能非常方便）**。这样的文件系统称为版本控制文件系统（versioning file system）**，因为它跟踪文件的不同版本。
    
    - **FS只保留文件的最新活版本。**
        - 定期查找文件数据，索引节点和其他结构的旧的死版本，并清理（clean）它们。
- 段的机制在释放中的使用
    
    - LFS清理程序按段工作，从而为后续写入清理出大块空间。我们预期清理程序读取M个现有段，将其内容打包（compact）到N个新段（其中N < M），然后将N段写入磁盘的新位置。然后释放旧的M段，文件系统可以使用它们进行后续写入。

# 43.10　确定块的死活

给定磁盘段S内的数据块D，LFS必须能够确定D是不是活的。为此，LFS会为描述每个块的每个段添加一些额外信息。具体地说，对于每个数据块D，LFS包括其inode号（它属于哪个文件）及其偏移量（这是该文件的哪一块）。该信息记录在一个数据结构中，位于段头部，称为段摘要块（segment summary block）。

段摘要⇒块上记录inode+偏移量T找到⇒imap⇒inode(可能已经在内存中)⇒读取T⇒查到对应块确定死活

```java
(N, T) = SegmentSummary[A]; 
inode = Read(imap[N]);
if (inode[T] == A)
    // block D is alive 
else
    // block D is garbage
```

下面是一个描述机制的图，其中段摘要块（标记为SS）记录了地址A0处的数据块，实际上是文件k在偏移0处的部分。通过检查imap的k，可以找到inode，并且看到它确实指向该位置。

![](Pasted%20image%2020250525215537.png)

LFS走了一些捷径，可以更有效地确定死活。例如，当文件被截断或删除时，LFS会增加其版本号（version number），并在imap中记录新版本号。通过在磁盘上的段中记录版本号，LFS可以简单地通过将磁盘版本号与imap中的版本号进行比较，跳过上述较长的检查，从而避免额外的读取。

[用最小的元数据变动快速判断数据是否有效](#用最小的元数据变动快速判断数据是否有效)

# 43.11　策略问题：要清理哪些块，何时清理

在上述机制的基础上，LFS必须包含一组策略，以确定何时清理以及哪些块值得清理。确定何时清理比较容易。要么是周期性的，要么是空闲时间，要么是因为磁盘已满。

空闲时间，要么是因为磁盘已满。确定清理哪些块更具挑战性，并且已成为许多研究论文的主题。在最初的LFS论文[RO91]中，作者描述了一种试图分离冷热段的方法。热段是经常覆盖内容的段。因此，对于这样的段，最好的策略是在清理之前等待很长时间，因为越来越多的块被覆盖（在新的段中），从而被释放以供使用。相比之下，冷段可能有一些死块，但其余的内容相对稳定。因此，作者得出结论，**应该尽快清理冷段，延迟清理热段**，并开发出一种完全符合要求的试探算法。但是，与大多数政策一样，这只是一种方法，当然并非“最佳”方法。后来的一些方法展示了如何做得更好[MR+97]。

# 43.12　崩溃恢复和日志

在更新期间崩溃对于文件系统来说是棘手的，因此LFS也必须考虑这些问题。

[崩溃恢复](#崩溃恢复)
# 43.13　小结

LFS引入了一种更新磁盘的新方法。LFS总是写入磁盘的未使用部分，然后通过清理回收旧空间，而不是在原来的位置覆盖文件。这种方法在数据库系统中称为影子分页（shadow paging）[L77]，在文件系统中有时称为写时复制（copy-on-write），可以实现高效写入，因为LFS可以将所有更新收集到内存的段中，然后按顺序一起写入。

这种方法的缺点是它会产生垃圾。旧数据的副本分散在整个磁盘中，如果想要为后续使用回收这样的空间，则必须定期清理旧的数据段。清理成为LFS争议的焦点，对清理成本的担忧[SS+95]可能限制了LFS开始对该领域的影响。然而，一些现代商业文件系统，包括NetApp的WAFL [HLM94]、Sun的ZFS [B07]和Linux btrfs [M07]，采用了类似的写时复制方法来写入磁盘，因此LFS的知识遗产继续存在于这些现代文件系统中。特别是，WAFL通过将清理问题转化为特征来解决问题。通过快照（snapshots）提供旧版本的文件系统，用户可以在意外删除当前文件时，访问到旧文件。

> 提示：将缺点变成美德每当你的系统存在根本缺点时，请看看是否可以将它转换为特征或有用的功能。NetApp的WAFL对旧文件内容做到了这一点。通过提供旧版本，WAFL不再需要担心清理，还因此提供了一个很酷的功能，在一个美妙的转折中消除了LFS的清理问题。系统中还有其他这样的例子吗？毫无疑问还有，但你必须自己去思考，因为本章的内容已经结束了。

# 附录

## 短寻道

**寻道 (seek)**

指磁盘的磁头（read/write head）从当前位置移动到目标磁道（track）的过程。

**短寻道**

则指磁头只需在相邻或接近的磁道之间移动。

### **原理**

机械硬盘 (HDD) 由盘片 (platter) 和磁头组成：

- 数据存储在盘片表面的同心圆轨道（track）上。
- 当要访问不同轨道的数据时，机械臂必须移动磁头到目标轨道，称为 **寻道操作**。

---

### **影响**

- 短寻道比长寻道快，但仍然是硬盘 I/O 的主要瓶颈之一。
- 当频繁出现短寻道（如频繁更新元数据和数据块）时，会显著降低吞吐量。

一个 HDD 的平均寻道时间（average seek time）通常在：

- 高端企业盘：~3-5 ms。
- 普通消费盘：~8-12 ms。

## **旋转延迟（rotational delay / rotational latency）**

### **定义**

> 旋转延迟是指盘片旋转到目标扇区正好在磁头下方所需的平均时间。

### **原理**

盘片不停以固定转速旋转（例如 7200 RPM、10000 RPM、15000 RPM）。

即便磁头已经移动到目标轨道上，磁头下的数据可能还没转到正确位置，需要再等半圈或更少。

平均旋转延迟计算公式：

例如：

- 7200 RPM = 120 RPS → 平均旋转延迟 ≈ 1 / (2 × 120) ≈ 4.17 ms。
- 15000 RPM → 平均旋转延迟 ≈ 2 ms。
# 附录
## 用最小的元数据变动快速判断数据是否有效
- 每个文件/对象在 inode 中都带一个 **version number**。
- **不是每个块、每个 inode、每个目录都要频繁改版本号。**
- 只有在文件对象发生“重大生命周期事件”（如截断、删除、重建）时，才会全局增加它的版本号。

### **版本号的基本设计**

- 每个文件/对象在 inode 中都带一个 **version number**。
    
- 当文件发生截断（truncate）、删除（unlink）、重建（recreate）时：
    
    → LFS 增加 inode 的 version number。
    
- 最新版本的 version number 被记录在 **imap** 中。
    

### **具体内容：段摘要记录什么？**

每个段（segment）通常包含：

- 实际写入的数据块（file blocks, indirect blocks, inode blocks）。
- 段摘要块（segment summary block）：
    - 描述段内每个数据块的元信息，包括：
        - **类型**（数据块 / inode 块 / indirect 块）
        - **文件 inode 号**
        - **数据块在文件中的偏移（逻辑块号）**
        - **版本号（version number）**
        - **校验和、标志位等**

### **为什么段摘要要存版本号？**

这是为了优化 **cleaner（段清理器）** 的工作：

- 当 cleaner 扫描段并评估哪些块是活跃的、哪些是无效的时：
    
    → 它可以直接用段摘要里的 **version number** 和当前 **imap 中的最新版本号** 做比较。
    
    → 如果段摘要里的版本号 **落后于 imap**，说明：
    
    ❌ 该块属于旧版本文件，不再有效，可以直接丢弃。
    

这种机制：

避免 cleaner 为每个块都去读取 inode

避免逐层查询目录树

大幅降低垃圾回收的 I/O 成本
## 崩溃恢复
### **LFS 在写入期间可能的崩溃点**

在正常操作中，LFS 会做两件关键事：

1. **将缓冲的脏数据写入新的段（segment）**
2. **定期更新检查点区域（checkpoint region, CR）**

这两者在写入期间都可能发生崩溃，例如：

- 写入段一半崩溃 → 段可能部分写入。
- 写入检查点一半崩溃 → 可能找不到新段链。

问题是：

> 系统重启时，如何确保文件系统一致，避免元数据混乱或数据丢失？

---

###  **LFS 崩溃恢复的核心设计：检查点与段链**

LFS 主要依赖两层防线：

✅ **段链（segment chain）**

✅ **检查点区域（checkpoint region, CR）**

---

### **段链的自洽性**

每个段都有：

- 自己的段摘要（segment summary block），包含段内内容、指向下一个段的指针。
- 这些段按顺序形成 **日志链（log chain）**。

👉 当新段写入时：

- LFS 先写完整个段（包括数据块、inode 块、imap 更新、段摘要）。
- **最后一步** 才更新 CR 里的段头/段尾指针。

因此：

- 如果在写段过程中崩溃，段尚未被 CR 记录，重启后会被完全忽略。
- 如果段完全写完但 CR 更新前崩溃，重启后用旧 CR 恢复，不会指向该段。

换句话说：

✅ 崩溃恢复只认 CR 指向的段链。

---

### **检查点区域的双写**

LFS 使用了一个经典的冗余设计：

- 检查点区域（CR）有两个副本。
- 每次更新，只写其中一个（交替写入）。
- 在写完前，另一个副本仍然有效。

因此：

✅ 即使崩溃发生在 CR 更新过程中，重启时：

- 系统可以用最后一个成功写入的 CR 恢复。
- 由于 CR 指向最新的段头和尾，只恢复到上一次 checkpoint。

这是 **单调一致的设计**，即：

> 磁盘上的 CR 永远不会指向半写入的段。

---

### **恢复流程简述**

重启时，LFS 恢复步骤：

1. 从两个 CR 副本中找到**最近的有效副本。（日志的结尾）**
2. 按 CR 指向的段链，从段摘要中重建：
    - inode map（imap）
    - 活跃 inode、目录、文件数据
3. 忽略段链之外的段（那些崩溃时未提交的段）。

这种设计天然保证：

✅ 崩溃后一致性（crash consistency）

✅ 无需回滚（undo），只需前滚（redo）

---

### **小细节：段内部的部分写入**

磁盘写入是以块为单位的，因此段的写入通常：

- 被设计成 **块对齐、段对齐**。
- 部分块失败 → 整段标记为无效。

此外，段内还可能有校验和（checksum），用于判断段是否写完整。

---

## 总结表

|崩溃位置|恢复策略|
|---|---|
|写入段中崩溃|CR 未更新 → 段不在段链中，重启时直接丢弃|
|写入 CR 中崩溃|使用另一个有效 CR 副本|
|段部分块写入|校验和不符，整个段无效|
|重启恢复时的核心依赖|CR + 段摘要，完全不依赖 RAM 中缓存|