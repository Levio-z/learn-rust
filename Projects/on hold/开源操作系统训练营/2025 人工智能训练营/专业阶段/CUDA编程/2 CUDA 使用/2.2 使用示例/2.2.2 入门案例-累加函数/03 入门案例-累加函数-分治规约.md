sum_gpu02_reduce_warp.cu
### 测试
数据大小：const size_t SIZE = 1 << 20; // 元素总数 half个数
```
nvcc sum_gpu/sum_gpu02_reduce_warp.cu -o target/sum_gpu02_reduce_warp

nsys profile -t cuda,nvtx,osrt -o target/sum_gpu02_reduce_warp -f true target/sum_gpu02_reduce_warp

nsys stats target/sum_gpu02_reduce_warp.nsys-rep

./target/sum_gpu02_reduce_warp

```

结果：
```
GPU result: 28.0194
CPU result: 524282

 Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)  Max (ns)   StdDev (ns)           Name         
 --------  ---------------  ---------  -----------  -----------  --------  ---------  -----------  ----------------------
     98.4        239971415          2  119985707.5  119985707.5    786641  239184774  168572936.5  cudaMalloc            
      0.6          1379086          1    1379086.0    1379086.0   1379086    1379086          0.0  cuLibraryLoadData     
      0.5          1180248          3     393416.0     111042.0    107422     961784     492224.5  cudaMemcpy            
      0.2           583266          2     291633.0     291633.0    256242     327024      50050.4  cudaFree              
      0.2           553818          1     553818.0     553818.0    553818     553818          0.0  cudaLaunchKernel      
      0.1           189337          1     189337.0     189337.0    189337     189337          0.0  cudaDeviceSynchronize 
      0.0              962          1        962.0        962.0       962        962          0.0  cuModuleGetLoadingMode
      0.0              894          1        894.0        894.0       894        894          0.0  cuLibraryGetKernel    
      0.0              641          1        641.0        641.0       641        641          0.0  cuKernelGetName  
```


### 并行累加原子操作性能提升
疑问： 
◆ 那怎么提升…原子操作不是必须的吗？ 

答案： 
◆ 是，但… 

思考： 
◆ 真的每个线程都需要用原子操作吗？ 
◆ 回想学过的数据结构 & 算法课…


◆ 如果我分治呢…？
- 并不 需要每个线程都直接加 到最终的 result 里 
- → 规约！
### 并行累加——分治规约

◆ Lane: 一个 warp 中的一个线程 
◆ grid-stride 循环规约出跨总线程数的 partial sum 
◆ 每个 warp 内选一个线程（第一个）进行循环累加 
◆ Warp 之间通过 atomicAdd 累加


```c++
// 核函数定义
template <typename T>

__global__ void reduce_warp_global_kernel(T *output, const T *input, size_t n) {

    size_t tid = threadIdx.x;

    unsigned int lane_id = tid % 32;

    size_t idx = blockIdx.x * blockDim.x + tid;

  

    // Lane 0 线程负责累加 warp 内的部分和

    if (lane_id == 0) {

        T warp_sum = 0;

        // 遍历 warp 内 32 个线程对应的全局索引

        for (int i = 0; i < 32; ++i) {

            // 跨步访问全局内存，步长是总线程数（gridDim.x * blockDim.x）

            for (int j = idx + i; j < n; j += blockDim.x * gridDim.x) {

                warp_sum += input[j];

            }

        }

        // wrap之间原子操作，将 warp 累加的和合并到全局 output

        atomicAdd(output, warp_sum);

    }

}
```
也可以修改为一次循环遍历：
```cpp
template <typename T>
__global__ void reduce_warp_global_kernel(T *output, const T *input, size_t n) {
    // 1. 线程索引与 Warp 内车道 ID
    size_t tid = threadIdx.x;               // 线程在 Block 内的局部 ID
    unsigned int lane_id = tid % 32;       // 线程在 Warp 内的车道号（0~31）
    size_t idx = blockIdx.x * blockDim.x + tid;  // 线程的全局索引（跨 Block）

    // 2. 仅 Warp 内 lane_id=0 的线程参与归约核心逻辑
    if (lane_id == 0) {  
        T warp_sum = 0;  // 存储当前 Warp 归约后的部分和
        
        // step = 总线程数（Grid 维度 × Block 维度），即线程跨步间隔
        // gridDim.x→ Grid 中 Block 的数量（网格维度）
        // blockDim.x→ 每个 Block 中的线程数量（线程块维度）。 
        const size_t step = blockDim.x * gridDim.x;  
        // 计算需要处理的“完整 Warp 批次”：向上取整，确保覆盖所有可能的 j < n
        
        // n - idx这是当前线程“还剩下”可以处理的元素数量（从它的起点 `idx` 到数组末尾 `n-1` 的距离）。
        // 表示**要多少个 step 才能覆盖剩余数据**，也就是跨多少次批次。
        const size_t total_elements = ((n - idx) + step - 1) / step;  

        // 3. 遍历并累加当前 Warp 负责的所有数据
        for (size_t linear_idx = 0; linear_idx < total_elements * 32; ++linear_idx) {
            // segment = 当前处理的“逻辑批次”（每个批次对应 step 跨步）
            const size_t segment = linear_idx / 32;  
            // lane = 模拟 Warp 内其他线程的车道号（0~31）
            const size_t lane = linear_idx % 32;  
            // j = 全局索引：基址 idx + 批次偏移(segment*step) + 车道偏移(lane)
            const size_t j = idx + segment * step + lane;  

            if (j < n) {  // 防止越界（当 n 不是 step 整数倍时）
                warp_sum += input[j];  // 累加数据
            }
        }

        // 4. 原子操作合并结果到全局 output
        atomicAdd(output, warp_sum);  
    }
}
```
const size_t total_elements = ((n - idx) + step - 1) / step;  
- 求idx开始之后每10个里面是不是有一个元素
- 为什么+ step - 1
	- 因为每10个都是idx负责的，最后一个差10个的如果不加上，就因为整除会被约去小数而无法正常统计

- 批次：total_elements是将数据平分给每个线程来说，一共要处理多少批次
- 总工作次数：一个线程计算代表了32个线程，所以总工作次数total_elements * 32
- 清晰计算出当前工作是属于第几批次，第几线程
	- 在总工作次数中，再去计算出当前批次的数据，更清晰 /32 计算出属于第几批次
	-  linear_idx % 32;  批次内谁处理
	- + segment * step 计算出批次偏移
	- lane批次内偏移
一次循环的优点
- **扁平化** → 简化循环结构，逻辑更清晰。
- **编译器优化更容易** → 单层循环利于 unroll、向量化（特别是 LLVM/Clang + NVCC）。
	- ⬇️ 编译器可能直接生成 32 条 `ld + add`，没有循环跳转。
	- Vectorization / Memory Coalescing（访存合并）
		-  索引表达式更线性，更容易触发 **访存合并**；
	- Loop Invariant Code Motion（循环不变式外提）
		- `%32` 和 `/32` 的开销几乎为零（移位/按位操作）；
		- 而双层循环需要额外维护两个迭代器（i 和 j），寄存器压力更大。
		-  只需要一个循环计数器 `linear_idx`
- **访存模式更规整** → 对 cache/TLB 友好，有机会让 L2 cache 命中率更高。
- **统一索引公式** → 不再区分内外层循环，推导/调试时减少思维负担。
- **便于扩展** → 如果以后 warp size 不是 32（比如新架构），修改更容易。
### 扩展
◆ 当然，可以再稍微优化一下实现… 
◆ 32 这里写死的，理论上最好获取实际值 
◆ 怎么获取？回忆第一节课 
→ cudaDeviceProp! → warpSize
### 分析
![](asserts/Pasted%20image%2020250820093433.png)
◆ 性能提升了很多！~30x 
◆ 提升的原理？ 
◆ 原来每个线程都做原子操作导致串行化
◆ 现在**每个 warp 都有一个线程在独立工作**，**原子操作 数量从原来的 线程数 变成 warp 数** （1/32）→ 提升 了并行度 
◆ 哦哦…明白了！那我让每个 block 都规约，还能减！

### 使用block级别规约

![](asserts/Pasted%20image%2020250820093521.png)

#### 性能降低了
![](asserts/Pasted%20image%2020250820093551.png)
#### ncu分析
![](asserts/Pasted%20image%2020250820093602.png)

#####  两者 compute % 都比 memory % 高

![](../../../asserts/Pasted%20image%2020250813150059.png)
◆ Roofline 相比于之前的加法 AI (~0.12) 更高，更靠近脊点
##### 单个线程的计算量增加太大，利用率/效率变低
◆ Intra-block 的比 intra-warp 的版本计算利用率和活跃 warp 数都要低不少 
→ 虽然减少了原子累加的操作数，**但单个线程的计算量增加太大，利用率/效率变低** 
- 一个线程要256个，原子累加不是瓶颈，每个线程都需要执行这个串行化操作256次
→ 是否意味着 warp > block？不需要 block 级的规约？
### 加大分治粒度
问题：是否这就是最优？如果不是，如何提升？ 
◆ Compute % > memory %，而且需要提高利用率 
◆ **分治规约…那能不能加大分治粒度？** 
◆ 可以，但线程怎么做到 warp/block 内共享/通信？