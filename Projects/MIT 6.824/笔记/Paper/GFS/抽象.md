我们设计并实现了 Google 文件系统，这是一个可扩展的分布式文件系统，适用于大型分布式数据密集型应用程序。它在廉价的商用硬件上运行时提供容错能力，并为大量客户端提供高聚合性能。

虽然与以前的分布式文件系统有许多相同的目标，但我们的设计是由对当前和预期的应用程序工作负载和技术环境的观察驱动的，这反映了与一些早期文件系统假设的明显背离。这促使我们重新审视传统选择并探索截然不同的设计点。

它在 Google 内部广泛部署，作为存储平台，用于生成和处理我们服务使用的数据以及需要大量数据集的研发工作。迄今为止最大的集群在一千多台机器上的数千个磁盘上提供数百 TB 的存储空间，并且由数百个客户端同时访问。

在本文中，我们介绍了旨在支持分布式应用程序的文件系统接口扩展，讨论了我们设计的许多方面，并报告了来自微基准测试和实际使用的测量结果。

### 关键字
容错、可扩展性、数据存储、集群存储
### 介绍

我们设计并实施了 Google 文件系统 （GFS），以满足 Google 快速增长的数据处理需求。GFS 与以前的分布式文件系统有许多相同的目标，例如性能、可伸缩性、可靠性和可用性。然而，它的设计是由对我们的应用程序工作负载和技术环境的关键观察（包括当前和预期的）驱动的，这反映了与一些早期文件系统设计假设的明显背离。我们重新审视了传统的选择，并探索了设计空间中截然不同的点。

首先，**组件故障是常态而不是例外**。 该文件系统由数百甚至数千台由廉价商品部件构建的存储机组成，并可由相当数量的客户端计算机访问。 组件的数量和质量几乎保证了某些组件在任何给定时间都无法运行，有些**组件将无法从当前故障中恢复。我们已经看到了由应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源故障引起的问题**。因此，**持续监控、错误检测、容错和自动恢复必须是系统不可或缺的一部分。**

其次，**按照传统标准，文件是巨大的**。多 GB 文件很常见。每个文件通常包含许多应用程序对象，例如 Web 文档。当我们经常使用包含数十亿个对象的许多 TB 的快速增长的数据集时，即使文件系统可以支持它，管理数十亿个大约 KB 大小的文件也是笨拙的。因此，必须重新审视设计假设和参数，例如 I/O 操作和块大小。

第三，**大多数文件通过附加新数据而不是覆盖现有数据来改变**。文件中的随机写入实际上不存在。 **写入后，文件仅被读取，而且通常只按顺序读取。 各种数据具有这些特征**。有些可能构成数据分析程序扫描的大型存储库。有些可能是正在运行的应用程序不断生成的数据流。有些可能是档案数据。有些可能是在一台机器上产生并在另一台机器上处理的中间结果，无论是同时还是稍后。鉴于对大文件的这种访问模式，附加成为性能优化和原子性保证的重点，而在客户端中缓存数据块则失去了吸引力。

第四，共同设计应用程序和文件系统 API 通过提高我们的灵活性来使整个系统受益。

例如，我们放宽了 GFS 的一致性模型，以极大地简化文件系统，而不会给应用程序带来沉重的负担。 我们还引入了原子追加操作，以便多个客户端可以同时追加到一个文件，而无需它们之间进行额外的同步。本文稍后将更详细地讨论这些内容。

当前部署了多个 GFS 集群用于不同的目的。最大的拥有 1000 多个存储节点和超过 300 TB 的磁盘存储，并且由不同机器上的数百个客户端连续大量访问。



