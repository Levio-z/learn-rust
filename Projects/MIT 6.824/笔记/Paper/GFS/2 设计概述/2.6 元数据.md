主服务器存储三种主要类型的元数据：文件和块命名空间、从文件到块的映射以及每个块的副本的位置。所有元数据都保存在主服务器的内存中。**前两种类型（命名空间和文件到块映射）也通过将突变记录到存储在主服务器本地磁盘上并在远程计算机上复制的作日志来保持持久性**。使用日志使我们能够简单、可靠地更新主状态，并且在主崩溃时不会冒不一致的风险。**主服务器不会持久存储块位置信息。相反，它会在主启动时以及每当 chunkserver 加入集群时询问每个 chunkserver 的块**。

### 2.6.1 内存中数据结构
由于元数据存储在内存中，因此主操作速度很快。此外，主设备在后台定期扫描其整个状态既简单又高效。这种定期扫描用于实现块垃圾收集、在存在块服务器故障时重新复制以及块迁移以平衡负载和磁盘空间此外，跨块服务器的使用。第 4.3 节和第 4.4 节将进一步讨论这些活动。

**这种纯内存方法的一个潜在问题是块的数量以及整个系统的容量受到主服务器内存量的限制**。这在实践中不是一个严重的限制。主节点为每个 64 MB 块维护少于 64 字节的元数据。大多数块都是满的，因为大多数文件包含许多块，只有最后一个块可能被部分填充。同样，文件命名空间数据通常需要每个文件少于 64 字节，因为它使用前缀压缩紧凑地存储文件名。

如果有必要支持更大的文件系统，则为主服务器添加额外内存的成本只是我们通过将元数据存储在内存中获得的简单性、可靠性、性能和灵活性所付出的很小的代价。

### 2.6.2 块位置

**主服务器不会保留哪些块服务器具有给定块的副本的持久记录。它只是在启动时轮询块服务器以获取该信息。此后，主服务器可以保持自身最新状态，因为它控制所有块放置并通过定期 HeartBeat 消息监控块服务器状态。**

我们最初尝试将块位置信息持久保存在主服务器上，但我们认为在启动时从块服务器请求数据要简单得多，此后定期请求数据要简单得多。这消除了在块服务器加入和离开集群、更改名称、失败、重新启动等时保持主服务器和块服务器同步的问题。在拥有数百台服务器的集群中，这些事件经常发生。

理解此设计决策的另一种方法是认识到，**块服务器对它自己的磁盘上有或没有哪些块拥有最终决定权**。尝试在主服务器上保持一致的此信息视图是没有意义的，因为块服务器上的错误可能会导致块自发消失（例如，磁盘可能坏了并被禁用），或者操作员可能会重命名块服务器。

### 2.6.3 操作日志

**操作日志包含关键元数据更改的历史记录**。它是 GFS 的核心。它**不仅是元数据的唯一持久记录，而且还充当定义并发作顺序的逻辑时间线**。 **文件和块及其版本（参见第 4.5 节）都通过创建它们的逻辑时间进行唯一且永久的标识。**

由于操作日志至关重要，因此我们必须可靠地存储它，并且在元数据更改持久化之前，不要使更改对客户端可见。否则，即使块本身幸存下来，我们实际上也会丢失整个文件系统或最近的客户端操作。 因此，我们将它复制到多台远程计算机上，并且只有在本地和远程将相应的日志记录刷新到磁盘后才响应客户端操作。主批处理在刷新之前将多个日志记录放在一起，从而减少刷新和复制对整体系统吞吐量的影响。

主服务器通过重放作日志来恢复其文件系统状态。为了最大限度地减少启动时间，我们必须保持日志较小。**每当日志增长超过一定大小时，主节点都会检查其状态，以便它可以通过从本地磁盘加载最新的检查点并仅重放之后的日志记录数量有限。检查点采用类似 B 树的紧凑形式，可以直接映射到内存中并用于命名空间查找，而无需额外的解析。这进一步加快了恢复速度并提高了可用性。**

由于构建检查点可能需要一段时间，因此主节点的内部状态的结构使得可以在不延迟传入突变的情况下创建新检查点。**主服务器切换到新的日志文件，并在单独的线程中创建新的检查点。新检查点包括切换前的所有突变。对于包含几百万个文件的集群，它可以在一分钟左右创建。完成后，它会在本地和远程写入磁盘。**

**恢复只需要最新的完整检查点和后续日志文件。** 较旧的检查点和日志文件可以自由删除，但我们保留了一些以防范灾难。 检查点期间的故障不会影响正确性，因为恢复代码会检测并跳过不完整的检查点。








