MapReduce 模型可以有多种不同的实现方式。如何正确选择取决于具体的环境。
	例如，一种实现方式适用于小型的共享内存方式的机器，另外一种实现方式则适用于大型 NUMA 架构的多处理器的主机，而有的实现方式更适合大型的网络连接集群。 本章节描述一个适用于 Google 内部广泛使用的运算环境的实现：用以太网交换机连接、由普通 PC 机组成的大型集群。在我们的环境里包括： 
1. x86 架构、运行 Linux 操作系统、双处理器、2-4GB 内存的机器。 
2. 普通的网络硬件设备，每个机器的带宽为百兆或者千兆，但是远小于网络的平均带宽的一半。 
3. 集群中包含成百上千的机器，因此，机器故障是常态。
4. 存储为廉价的内置 IDE 硬盘。一个内部分布式文件系统用来管理存储在这些磁盘上的数据。文件系统通过数据复制来在不可靠的硬件上保证数据的可靠性和有效性。
5. 用户提交工作（job）给调度系统。每个工作（job）都包含一系列的任务（task），调度系统将这些任 务调度到集群中多台可用的机器上。
#### comment：
**MapReduce 是一种模型，不是特定实现。**

实现方式取决于部署环境：

| 环境类型              | 特点与对应实现风格                   |
| ----------------- | --------------------------- |
| 共享内存（如单机）         | 可用线程或协程实现 MapReduce，避免序列化成本 |
| NUMA 多处理器主机       | 需考虑缓存一致性和 NUMA-aware 任务调度   |
| 网络连接的大型集群（Google） | 面向故障恢复、容错与带宽优化的调度与分布式 I/O   |

 1. **硬件配置：x86 双处理器、2–4GB 内存的普通 PC**
	- 使用**廉价硬件**构建高性价比系统（commodity hardware）
	- 不追求高可靠性硬件，而是通过**软件层容错机制**来弥补硬件不可靠
2. **网络带宽瓶颈**
	- 假设**网络瓶颈是常态**
	- 因此 MapReduce 的调度会避免过多的跨机通信：
		- **本地性调度**（Data Locality）：优先将任务安排到数据所在的机器上执行
		- **合并优化**（Combiner）：在 map 端进行部分合并，减少网络传输负载
- 3. **机器故障是常态**
	- “集群中包含成百上千的机器，因此，机器故障是常态。”
	- 应对策略：
		- **任务失败自动重试**
		- **中间结果备份存储**
		- **Master 监控所有任务进度和健康状态**

**设计哲学：硬件便宜出故障不可怕，软件层容错就行**
```
 Map Task A1 ─┬──\
              │   Reduce R1
 Map Task A2 ─┘   ──────────► Final Output
      ↓                ▲
 [失败重试]      [多副本中间数据备份]
```

4. **廉价内置 IDE 硬盘 + 分布式文件系统**
- Google 使用的是 **GFS（Google File System）**：
	- 提供**跨多机的数据冗余（3 副本）**
	- 提供**容错、恢复、并发访问机制**
	- MapReduce 的输入/输出都通过 GFS 管理
5. **作业调度系统（job/task scheduling）**
	- 用户提交作业（job）描述 Map + Reduce 函数
	- 系统自动将作业划分为多个 map/reduce **任务（task）**
	- 调度器根据数据位置、机器负载、任务失败情况动态调度
调度优化策略：

|策略|描述|
|---|---|
|**数据局部性优先**|优先调度到拥有输入数据的机器上执行|
|**空闲节点填补**|用任务填补空闲资源提升利用率|
|**失败重试**|若任务失败，调度到其他节点重新执行|
三、为什么这个架构如此关键？
- **鲁棒性**：面对成百上千台机器，**节点宕机不影响整体执行**，这是 MapReduce 架构极大的工程优势。
- **高扩展性**：可以轻松横向扩展——加入更多普通机器即可。
- **自动容错 + 调度**：将并行性、错误处理、负载均衡等复杂性封装起来，让开发者专注业务逻辑。
- 需考虑缓存一致性和 NUMA-aware 任务调度
- **中间结果备份存储**
### 3.1 执行概括
通过将 Map 调用的输入数据自动分割为 M 个数据片段的集合，Map 调用被分布到多台机器上执行。输 入的数据片段能够在不同的机器上并行处理。使用分区函数将 Map 调用产生的中间 key 值分成 R 个不同分区（例如，hash(key) mod R），Reduce 调用也被分布到多台机器上执行。分区数量（R）和分区函数由用户来指定。
[03-1 MapReduce 在并行计算中如何划分任务与调度 Reduce 任务的关键机制](03-1%20MapReduce%20在并行计算中如何划分任务与调度%20Reduce%20任务的关键机制.md)
![](Pasted%20image%2020250615213936.png)
图中展示了我们的 MapReduce 实现中操作的全部流程。当用户调用 MapReduce 函数时，将发生下面的一 系列动作（下面的序号和图 1 中的序号一一对应）：
1. 用户程序首先调用的 MapReduce 库将输入文件分成 M 个数据片度，每个数据片段的大小一般从 16MB 到 64MB(可以通过可选的参数来控制每个数据片段的大小)。然后用户程序在机群中创建大量的程序副本。
2. 这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是 worker 程序，由 master 分配 任务。有 M 个 Map 任务和 R 个 Reduce 任务将被分配，master 将一个 Map 任务或 Reduce 任务分配 给一个空闲的 worker。 
3. 被分配了 map 任务的 worker 程序读取相关的输入数据片段，从输入的数据片段中解析出 key/value pair，然后把 key/value pair 传递给用户自定义的 Map 函数，由 Map 函数生成并输出的中间 key/value pair，并缓存在内存中。 
4. 缓存中的 key/value pair 通过分区函数分成 R 个区域，之后周期性的写入到本地磁盘上。缓存的 key/value pair 在本地磁盘上的存储位置将被回传给 master，由 master 负责把这些存储位置再传送给 Reduce worker。
5. 当 Reduce worker 程序接收到 master 程序发来的数据存储位置信息后，使用 RPC 从 Map worker 所在 主机的磁盘上读取这些缓存数据。当 Reduce worker 读取了所有的中间数据后，通过对 key 进行排序 后使得具有相同 key 值的数据聚合在一起。由于许多不同的 key 值会映射到相同的 Reduce 任务上， 因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。
	- commit：Reduce worker 启动时，会从所有参与的 Map 任务机器上，通过 RPC 远程读取属于自己分区的所有中间数据文件。
	- [03-2 外部排序](03-2%20外部排序.md)
6. Reduce worker 程序遍历排序后的中间数据，对于每一个唯一的中间 key 值，Reduce worker 程序将这 个 key 值和它相关的中间 value 值的集合传递给用户自定义的 Reduce 函数。Reduce 函数的输出被追加到所属分区的输出文件。 
7. 当所有的 Map 和 Reduce 任务都完成之后，master 唤醒用户程序。在这个时候，在用户程序里的对 MapReduce 调用才返回。

在成功完成任务之后，MapReduce 的输出存放在 R 个输出文件中（对应每个 Reduce 任务产生一个输出 文件，文件名由用户指定）。一般情况下，用户不需要将这 R 个输出文件合并成一个文件–他们经常把这些文 件作为另外一个 MapReduce 的输入，或者在另外一个可以处理多个分割文件的分布式应用中使用。
### 3.2 Master 数据结构
Master 持有一些数据结构，它存储每一个 Map 和 Reduce 任务的状态（空闲、工作中或完成)，以及 Worker 机器(非空闲任务的机器)的标识。 Master 就像一个数据管道，中间文件存储区域的位置信息通过这个管道从 Map 传递到 Reduce。因此， 对于每个已经完成的 Map 任务，master 存储了 Map 任务产生的 R 个中间文件存储区域的大小和位置。当 Map 任务完成时，Master 接收到位置和大小的更新信息，这些信息被逐步递增的推送给那些正在工作的 Reduce 任 务。