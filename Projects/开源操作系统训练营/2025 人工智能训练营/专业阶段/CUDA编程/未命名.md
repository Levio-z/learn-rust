![](asserts/Pasted%20image%2020250813111031.png)


◆ 右移 →
提高 AI 
◆ 𝐴𝐼 = 𝑊/𝑄 
1. 提高 W； 
2. 降低 Q；
3. 提高 W + 降低 Q；

### 提高W
1. 算子融合 
2. 没有其他可融合的操作的话，多加几 个数？
	-  一个内核多做加法，以前是两个数 相加，现在把多个数同时累加？ 
	- → 累加求和

◆ 把一个连续数组里的多个数字（这里是 都是1）累加成一个数 
◆ 仍然使用模板 
◆ 加法太熟悉了，简单！ 
◆ 运行看看结果： 
◆ 啊？为什么错了？

![](asserts/Pasted%20image%2020250813111351.png)

数据大小：const size_t SIZE = 1 << 24; // 元素总数 half个数
```
nvcc sum_gpu/sum_gpu.cu -o target/sum_gpu
```

```
nsys profile -t cuda,nvtx,osrt -o target/sum_gpu -f true target/sum_gpu
```

```
nsys stats target/sum_gpu.nsys-rep
```

```
./target/sum_gpu
```

疑问：为什么错了？ 
答案：
◆ 因为这是并行！ 
◆ 多个线程同时进行写入操作！ 
◆ 但之前的加法也是并行啊？为什么能对？


![](asserts/Pasted%20image%2020250813111505.png)


![](asserts/Pasted%20image%2020250813111514.png)

答案：
◆ 因为之前的加法是尴尬并行（Embarrassingly Parallel） 
→ 之前的加法没有冲突/竞争的数据依赖问题

疑问：那要怎么解决这个竞争的问题？

答案： 
◆ 确保顺序执行，同时只能有一个写入操作 
**→ 原子操作**
→ “either run to completion in one go, or have no effect”

### 并行累加

![](asserts/Pasted%20image%2020250813111711.png)
◆ 将原来的加法换成原子累加操作 
◆ atomicAdd(): CUDA 支持的原子累加操作
◆ 答案对了！但… 
◆ 运行时间增加了很多，比原来在这个情况下慢了两百多倍！
疑问：为什么慢？ ◆ 看看 ncu

![](asserts/Pasted%20image%2020250813111854.png)

观察： 
◆ 计算与内存吞吐/利用率都低到离谱…
◆ 很多线程都在闲置 → 原子操作导致线程之间完全串行！
→ 回第一节课 → 看起来在并行但其实并没有在并行！

### 并行累加原子操作性能提升
疑问： 
◆ 那怎么提升…原子操作不是必须的吗？ 

答案： 
◆ 是，但… 

思考： 
◆ 真的每个线程都需要用原子操作吗？ 
◆ 回想学过的数据结构 & 算法课…


◆ 如果我分治呢…？
- 并不 需要每个线程都直接加 到最终的 result 里 
- → 规约！
### 并行累加——分治规约

◆ Lane: 一个 warp 中的一个线程 
◆ grid-stride 循环规约出跨总线程数的 partial sum 
◆ 每个 warp 内选一个线程（第一个）进行循 环累加 
◆ Warp 之间通过 atomicAdd 累加

```c++
// 核函数定义

template <typename T>

__global__ void reduce_warp_global_kernel(T *output, const T *input, size_t n) {

    size_t tid = threadIdx.x;

    unsigned int lane_id = tid % 32;

    size_t idx = blockIdx.x * blockDim.x + tid;

  

    // Lane 0 线程负责累加 warp 内的部分和

    if (lane_id == 0) {

        T warp_sum = 0;

        // 遍历 warp 内 32 个线程对应的全局索引

        for (int i = 0; i < 32; ++i) {

            // 跨步访问全局内存，步长是总线程数（gridDim.x * blockDim.x）

            for (int j = idx + i; j < n; j += blockDim.x * gridDim.x) {

                warp_sum += input[j];

            }

        }

        // wrap之间原子操作，将 warp 累加的和合并到全局 output

        atomicAdd(output, warp_sum);

    }

}
```

修改为一次循环遍历：
```cpp
template <typename T>
__global__ void reduce_warp_global_kernel(T *output, const T *input, size_t n) {
    // 1. 线程索引与 Warp 内车道 ID
    size_t tid = threadIdx.x;               // 线程在 Block 内的局部 ID
    unsigned int lane_id = tid % 32;       // 线程在 Warp 内的车道号（0~31）
    size_t idx = blockIdx.x * blockDim.x + tid;  // 线程的全局索引（跨 Block）

    // 2. 仅 Warp 内 lane_id=0 的线程参与归约核心逻辑
    if (lane_id == 0) {  
        T warp_sum = 0;  // 存储当前 Warp 归约后的部分和
        
        // step = 总线程数（Grid 维度 × Block 维度），即线程跨步间隔
        const size_t step = blockDim.x * gridDim.x;  
        // 计算需要处理的“完整 Warp 批次”：向上取整，确保覆盖所有可能的 j < n
        
        // n - idx这是当前线程“还剩下”可以处理的元素数量（从它的起点 `idx` 到数组末尾 `n-1` 的距离）。
        // 表示**要多少个 step 才能覆盖剩余数据**，也就是跨多少次批次。
        const size_t total_elements = ((n - idx) + step - 1) / step;  

        // 3. 遍历并累加当前 Warp 负责的所有数据
        for (size_t linear_idx = 0; linear_idx < total_elements * 32; ++linear_idx) {
            // segment = 当前处理的“逻辑批次”（每个批次对应 step 跨步）
            const size_t segment = linear_idx / 32;  
            // lane = 模拟 Warp 内其他线程的车道号（0~31）
            const size_t lane = linear_idx % 32;  
            // j = 全局索引：基址 idx + 批次偏移(segment*step) + 车道偏移(lane)
            const size_t j = idx + segment * step + lane;  

            if (j < n) {  // 防止越界（当 n 不是 step 整数倍时）
                warp_sum += input[j];  // 累加数据
            }
        }

        // 4. 原子操作合并结果到全局 output
        atomicAdd(output, warp_sum);  
    }
}
```

```
const size_t total_elements = ((n - idx) + step - 1) / step;  
```
100减去，第一个，索引就是数量-1。
((100-0)+9 )/= 10 = 10
一共有多少批次





◆ 当然，可以再稍微优化一下实现… 
◆ 32 这里写死的，理论上最好获取实际值 
◆ 怎么获取？回忆第一节课 
→ cudaDeviceProp! → warpSize

![](asserts/Pasted%20image%2020250813145242.png)

◆ 性能提升了很多！~30x 
◆ 提升的原理？ 
◆ 原来每个线程都做原子操作导致串行化
◆ 现在每个 warp 都有一个线程在独立工作，原子操作 数量从原来的 线程数 变成 warp 数 （1/32）→ 提升 了并行度 
◆ 哦哦…明白了！那我让每个 block 都规约，还能减！

![](asserts/Pasted%20image%2020250813150319.png)
![](asserts/Pasted%20image%2020250813145728.png)

![](asserts/Pasted%20image%2020250813145814.png)

观察： ◆ 两者 compute % 都比 memory % 高 →

◆ Intra-block 的比 intra-warp 的版本计算利用率和活跃 warp 数都要低不少 
→ 虽然减少了原子累加的操作数，**但单个线程的计算量增加太大，利用率/效率变低** 
→ 是否意味着 warp > block？不需要 block 级的规约？

![](asserts/Pasted%20image%2020250813150059.png)

◆ Roofline 相比于之前的加法 AI (~0.12) 更高，更靠近脊点

问题：是否这就是最优？如果不是，如何提升？ 
◆ Compute % > memory %，而且需要提高利用率 
◆ **分治规约…那能不能加大分治粒度？** 
◆ 可以，但线程怎么做到 warp/block 内共享/通信？

### 内存层级结构
◆ 解决方案：阶层结构 
◆ 越靠近上层/处理器/计算单元 →越快、越小、越贵 → 存放更常用的数据和指令 
◆ 越靠近下层 →越慢、 越大、越便宜 → 存放更不常用/更大规模的数据和指令

![](asserts/Pasted%20image%2020250813150954.png)

中间两个蓝色的是L2级缓存
![](asserts/Pasted%20image%2020250813151050.png)

![](asserts/Pasted%20image%2020250813151212.png)
https://ai.gopubby.com/memory-types-in-gpu-6373b7a0ca47?gi=06646da76c95

### CUDA内存模型
![](asserts/Pasted%20image%2020250813151737.png)
https://ai.gopubby.com/memory-types-in-gpu-6373b7a0ca47?gi=06646da76c95
- 本地内存逻辑上是一块物理内存

纹理
- TEX硬件单元


问题： 
◆ 加大分治粒度，但线程怎么做到 warp/block 内共享/通信？ 
答案： 
◆ 可以用共享内存！ 
◆ 数据在共享内存中对同块中的线程均可见

![](asserts/Pasted%20image%2020250813152408.png)

◆ 利用共享内存做树状规约 
◆ 树状规约的方式貌似有些…不习惯? 
◆ 这里使用动态分配共享内存 
◆ 核函数发射第三个参数加入动态分配的共享 内存大小 
◆ __shared__: 表示该变量分配至共享内存 
◆ __syncthreads(): 同步块内线程
- 保证上面的线程都进行了操作，同步函数


![](asserts/Pasted%20image%2020250813153000.png)


◆ 还有没有能继续优化的方法？ 
观察： 
◆ 树状规约一个问题：越往后**活跃的 线程越少，闲置的线程越多，利用 率下降** 
◆ 原子操作现在每个 block 一次，能 不能继续减少？


![](asserts/Pasted%20image%2020250813153200.png)

◆ 多了个奇怪的东西 
◆__shfl_down_sync(): **线程束洗牌函数 （Warp Shuffle Function）**
- 第一个mark，线程范围
- 从 lane_id + offset 处取 sum 的值 
特点
- 使用 register 无需共享内存和同步 → 高效 
-  超出边界反正本值 → 无需担心边界问题
	- 当通过 `lane_id + offset` 这个索引去读取 `sum` 的值时，如果计算出的索引超出了 `sum` 所在数据结构（比如数组）的有效边界范围，这种情况下依然会返回一个合理的 “默认值”（或符合预期的基础值），而不会因为索引越界导致程序出错（如崩溃、抛出异常等）。
![](asserts/Pasted%20image%2020250813154015.png)

◆ 相比于之前的版本，使用 __shfl_down_sync() 后 Roofline 显示 AI 明显提高了很多 
◆ 原因？因为__shfl_down_sync() 使得整理计算量增加，AI 增加
![](asserts/Pasted%20image%2020250813154055.png)

观察： 
◆ 确实有明显提升！ 
◆ 但跟目前最快的 smem-tree 规约实 现比还是差距很大，原因？
◆ 因为我们没对 warp 间规约优化！→ 将原来的 smem 手动树状规约换成 warp-shuffle 规约

![](asserts/Pasted%20image%2020250813154112.png)

◆ 抽象出一个使用 __shfl_down_sync() 的 warp_reduce() 函数 
◆ #pragma unroll: 暗示编译器进行循环展开 
◆ 前面跟之前一样，grid stride 循环 + warp 内 规约 
◆ 块内 Warp 间规约用 smem 进行一次 warp_reduce，然后将结果加到最终输出 
◆ 原子操作数：1/block

block 加 wrap规约
![](asserts/Pasted%20image%2020250813154328.png)


之前的另一个问题： 
◆ 原子操作现在每个 block 一次，能 不能继续减少？ 
答案：
◆ 还真能！

◆ 原子操作目前的作用是跨 Block 规约 

◆ 或许可以分级？两个核函数： 
◆ 第一个是目前的 Block 内规约 
◆ 第二个在第一个的基础上跨 Block 规约

◆ 理论可行，实践试试！


![](asserts/Pasted%20image%2020250813154417.png)
问题： 
◆ 第一个 kernel 的结果需要存到全局内存 → 需要分配一个临时数组 注意： 
◆ 第二个 kernel 的 gridSize 是 1 
- 只有一个block
◆ 为什么？
![](asserts/Pasted%20image%2020250813154602.png)
◆ 第一个 Kernel 逻辑和之前基本相同 ◆ 最后的块内规约从 原子操作跨 block 规约 → 直接赋值到中间数组

![](asserts/Pasted%20image%2020250813154611.png)

◆ 和第一个 Kernel 逻辑基本相同
◆ Grid Stride Loop → Block Stride Loop 
◆ 原子操作数：0 ！ 
◆ 测测性能如何

![](asserts/Pasted%20image%2020250813154631.png)

观察： 
◆ 性能反而降低了… 分析原因： 
◆ 因为分成了两个 kernel，有 kernel 发射、额外计算、访存以及隐式同 步的开销！

### 特性对比

Smem Atomic Reduce
Pros： ◆ 代码简单，一个 kernel Cons: ◆ 需要原子操作

Two-Pass Reduce
Pros： ◆ 无需原子操作 Cons: **◆ 额外核函数发射、隐式同步、计算和访存开销**

结论：数据量大、原子操作（半精度）占时高时用 Two-Pass




◆ 之前用了两个 kernel 来完成跨 block 规约，避免原子操作 
◆ 为什么不能像同步 块内/warp内 线程一样同步块间线程呢？
◆ 其实…可以！

◆ 但，你得是 CUDA 9.0+ 
◆ 用 Cooperative Group！
![](asserts/Pasted%20image%2020250813155029.png)

◆ 代码逻辑保持和之前的 two-pass 一样 
◆ block.sync(): __syncthreads() 
◆ grid.sync(): 跨 block 同步

既可以进行胯间同步
![](asserts/Pasted%20image%2020250813155256.png)

◆ 但有额外需要注意的点 
◆ cudaOccupancyMaxActiveBlocksPerMultiproce ssor(): 获取每SM对给定kernel和配置下最大的 线程块数量 
◆ cudaDevAttrCooperativeLaunch/prop.cooperati veLaunch 确保运行环境支持 Cooperative Group 
◆ cudaLaunchCooperativeKernel(): 发射使用 Cooperative Group 的核函数

![](asserts/Pasted%20image%2020250813155355.png)

观察： 
◆ 性能不错！比 Two-pass 快 
◆ 仅逊于之前的 Smem warp 规约实 现
◆ 明显，理论上这种情况应该数据量 
- 更大的时候优势会更大，看看：

◆ 换到之前的 x16 (i.e., ~16M) 观察： 
◆ Grid sync 性能成为最好而且优势 巨大！
◆ 确实数据量大的时候优势更明显
![](asserts/Pasted%20image%2020250813155447.png)

### Two-Pass vs. Grid Sync

Pros： 
◆ 相对简单，兼容任何版本 
◆ 支持任何 grid size 
Cons: 
◆ 更多核函数发射和额外信息/操作开销

Grid Sync

Pros： 
◆ 单个核函数，避免额外核函数发射开销 
◆ 结合其他 CG 功能允许更多细粒度操作
Cons:
◆ 较为复杂，注意事项较多，需仔细保证 前提成立，否则可能会导致死锁等行为 
◆ 无法支持任意 grid size

问题： 
◆ 之前这里的树状规约怎么方式为什么是 这样？ 
◆ 为什么不是以数组中点的对称树状规约? 
◆ 可以，我们换一下试试

![](asserts/Pasted%20image%2020250813155600.png)

观察： ◆ 怪了，怎么这样明显性能掉了？ ◆ 而且数据量越大，掉的越多 ◆ 分析用… ncu！

![](asserts/Pasted%20image%2020250813155633.png)


![](asserts/Pasted%20image%2020250813155644.png)

◆ ncu Summary 下总结了可提升性能的机会/发现的问题/瓶颈 
◆ 点击跳转到详细页面（Details） 
◆ 这些都提到这个核函数最大的性能问题是因为一个叫 Bank Conflict 的东西 
◆ 那是什么？


![](asserts/Pasted%20image%2020250813155747.png)

### Bank Conflict
◆ 共享内存被划分为 32 个 Bank，每个 bank 是 4 bytes 
◆ 回顾：一个 warp 有 32 个线程 
◆ 并非巧合！连续地址的数据按 Bank 大小循环映射到不同 Bank 
◆ 同一个 warp 的所有线程同时执行同一无冲突的指令，比如 ld/sw 4 bytes，1 cycle 完成
![](asserts/Pasted%20image%2020250813155853.png)


◆ 但当一个 warp 中的**多个线程同时（Instr./TX）访问同一个 bank 的不同地址**时，这时 候访问将会被串行化，这个现象就叫 Bank Conflict。重点：访问规律/模式 
◆ 如果多个线程同时访问同一个 bank 的同一个地址时，这时不会发生 bank conflict，而 是会直接发生广播 (Broadcast) 或多播 (Multicast)
- 一个循环完成

◆ 因此，Bank conflict 常发生于不连续访问共享内存的情况 
◆ 最常见的不连续访问情景就是跨步访问（比如刚才的树状规约） 
◆ 当然还有其他较为少见的情况，比如随机访问 
◆ 来分析一下刚才的情况



第一层规约：
```
T0: smem[0] 和 smem[1] → Bank 0, Bank1 
T1: smem[2] 和 smem[3] → Bank 1, Bank2
… 
T16: smem[32] 和 smem[33] → Bank 0, Bank1 ! 

Bank Conflict! (2-way)
```

◆ 好吧… 那 Bank conflict 怎么解决呢？ 
◆ 有多种办法，其中第一种方式是: 
1. 算法/访存模式优化

◆ 这个方法是遇到 bank conflict 的上上策，因为是最彻底、最根源性的解决方法 
◆ 比如刚才那个例子，我们就可以使用之前的对半向前的树状规约模式


第一层规约：

```
T0: smem[0] 和 smem[32] → Bank 0, Bank0 
T1: smem[1] 和 smem[33] → Bank 1, Bank1 … 
T31: smem[31] 和 smem[63] → Bank 31, Bank31
```

统一block中的不同线程才是冲突

![](asserts/Pasted%20image%2020250813160358.png)

◆ 那有时候算法上没法很好的替换和解决，怎么办？
1. 算法/访存模式优化 
2. 内存填充（Padding） 
3. ? ? ?

◆ 看一个例子，transpose（转置）核函 数

![](asserts/Pasted%20image%2020250813160531.png)


◆ 该实现是否有 Bank conflict？ 
◆ 有！
◆ 最后一行，进行存入的时候 
◆ 几路冲突？
### 消除冲突


![](asserts/Pasted%20image%2020250813160635.png)


◆ 为什么这样就没有 Bank conflict 了？？ 
◆ tile[threadIdx.x][threadIdx.y] 
◆ 2D 结构时会隐式计算线性索引： linear_idx = threadIdx.y * blockDim.x + threadIdx.x; 
◆ y → 行，外层；x → 列，内层 
◆ 同一 threadIdx.y 的时候，threadIdx.x 全部在访问同一个 Bank！


![](asserts/Pasted%20image%2020250813160727.png)

◆ 填充之后的逻辑视图相当于变成了 这样： 
◆ Bank 不再是一条竖线 
◆ 以 Bank 0 为例，填充后变成这样：
![](asserts/Pasted%20image%2020250813160804.png)
◆ 回归到物理视图，warp 的 访问就变成这样：
◆ 从这两个视图可以看出： 
◆ 确实没有 Bank conflict 了！ 
◆ 每个 warp 内都在访问不 同的 Bank！ 
◆ 好，我们看看 ncu 确认

![](asserts/Pasted%20image%2020250813161054.png)
![](asserts/Pasted%20image%2020250813164224.png)


◆ 嗯，ncu 显示这一 行确实有 bank conflict！ 
◆ L1 wavefront shared excessive 不为 0！


![](asserts/Pasted%20image%2020250813164325.png)






◆ 嗯？？ncu 在 source 里 的 L1 Wavefront Shared Excessive 显示为 0 
◆ 但下面 Details 里显示有 bank conflict！？
◆ 这是怎么回事，难道我们 分析错了？

访问物理设计上的

◆ 共享内存与 L1 缓存共享 物理空间！


◆ 懂了懂了，但有一个问题： 
◆ Padding 会额外消耗 smem 的空间，根据情景可能会浪费许多宝贵的 smem 空间 
◆ 有没有缓解方法？ 
1. 算法/访存模式优化 
2. 内存填充（Padding） 3. ? ? ?

◆ 解决 Bank conflict 的另一优雅技巧！ 
◆ AI 核心——矩阵乘法! 
◆ 思考：
1. Padding 可以解决所有的 bank conflict 情况吗？如果不能，有什么例子吗？（Hint）
2. 还有什么方法可以缓解或消除 bank conflict？
3. 共享内存和 L1 缓存共享物理空间，那相同的访问模式/规律是不是访问他们都是一样的 效率/性能？试试看