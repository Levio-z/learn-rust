![](asserts/Pasted%20image%2020250812150817.png)

![](asserts/Pasted%20image%2020250812150847.png)

![](asserts/Pasted%20image%2020250812150831.png)


![](asserts/Pasted%20image%2020250812150915.png)

- https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf

- GPU内存带宽


### 内存墙
◆ 摩尔定律：算力每18-24个月翻倍 
◆ 但 DRAM 每年 +~10% 
◆ 冯·诺依曼架构: **数据必须在处理器和内存之间频繁传输** 
◆ 数据量大或计算密度低时 → 内存瓶颈 
◆ 总结：处理器（CPU/GPU）计算速度远快于 内存访问速度！

![](asserts/Pasted%20image%2020250812151432.png)
https://arxiv.org/pdf/2403.14123

观察： 
◆ 19.5 TFLOPS = 𝟏. 𝟗𝟓 × 𝟏𝟎𝟏𝟑 𝒐𝒑/𝒔𝒆𝒄 
◆ 2039 GB/s = 𝟐. 𝟎𝟑𝟗 × 𝟏𝟎𝟏𝟐 𝒃𝒚𝒕𝒆𝒔/𝒔𝒆𝒄 
◆ 即便 80GB HBM2e，速度相对算力有限 
◆ 对于 float 加法而言： 
	◆ 12 bytes 内存访问（2读+1写） 
	◆ 1 op 
◆ → **本身需要的内存访问较多，而计算量很少**

◆ 重要发现：理论上加法的性能瓶颈在内存访问上 
◆ 如何直观快速的判断一个算子在某个硬件上的性能瓶颈？ 
◆ 如何量化性能瓶颈？
→ 有助于判断和优化

### Roofline模型
![](asserts/Pasted%20image%2020250812151909.png)

![](asserts/Pasted%20image%2020250812152011.png)
β最大内存带宽
I 最大计算强度

特性： 
◆ 硬件有影响——Hardware-dependent 

◆ 拐点：也叫脊点（ridge point），可达到该硬 件理论最高性能的最低计算强度 


◆ 脊点以左：访存密集型，瓶颈在访存 
◆ 脊点以右：计算密集型，瓶颈在计算

示例： 
◆ Float 加法: 1 𝐹𝐿𝑂𝑃 /12 𝑏𝑦𝑡𝑒𝑠 = 1 /12 ≈ 𝟎. 𝟎𝟖𝟑 𝑭𝑳𝑶𝑷 𝒃𝒚𝒕𝒆 
◆ A100： 
◆ 19.5 TFLOPS = 𝟏. 𝟗𝟓 × 𝟏𝟎𝟏𝟑 𝒐𝒑/𝒔𝒆𝒄 
◆ 2039 GB/s = 𝟐. 𝟎𝟑𝟗 × 𝟏𝟎𝟏𝟐 𝒃𝒚𝒕𝒆𝒔/𝒔𝒆𝒄 
◆ 𝐼𝑚𝑎𝑥 = 𝟏.𝟗𝟓×𝟏𝟎𝟏𝟑 𝑭𝑳𝑶𝑷/𝒔𝒆𝒄 𝟐.𝟎𝟑𝟗×𝟏𝟎𝟏𝟐 𝒃𝒚𝒕𝒆𝒔/s𝒆𝒄 ≈ 𝟗. 𝟓𝟔 𝑭𝑳𝑶𝑷 𝒃𝒚𝒕𝒆 

◆ 确实是访存密集型
◆ 加法是比较典型的访存密集型算子 
### 优化访存密集型很重要
◆ 计算密集型？→ GEMM，之后… 
◆ **访存密集型算子占大多数**，因为 
	◆ 内存墙 ：内存硬件和计算机架构导致算子很容易受到内存影响
	◆ 深度学习等领域数据量大 
◆ 因此，**优化访存密集型算子**很重要！ 

◆ 刚才的 Roofline 分析提供**理论基础**，实际情况更为复杂，实际达到的性能还取决于其他因 素…（e.g., 代码实现）
- 算子本身不够效率
- 时钟频率？


### Nsight Compute
使用完整路径名
```
 sudo /usr/local/cuda/bin/ncu --print-details all --nvtx --call-stack --set full ./target/add_gpu02
```
[3 修改性能计数器权限](chatgpt/3%20修改性能计数器权限.md)
- WSL2 支持 GPU 加速（NVIDIA CUDA），但很多硬件层面的调试和性能分析功能受限。
- Nsight Compute 在 WSL 里只能做最基本的分析，不能访问底层硬件计数器。
- 这就是你看到 `ERR_NVGPUCTRPERM` 的根本原因，无法解决。

◆ 想分析之前的 GPU 并行加法代码 
◆ → Nsight Compute (ncu) ：英伟达的内核级性能分析工具 
◆ 使用 ncu CLI ：
```
sudo ncu --print-details --nvtx --call-stack --set full ./add_cuda
```
◆ 导出文件并使用 ncu GUI：
```
sudo ${ncu} --nvtx --call-stack --set full -f --export add_cuda.ncu-rep ./add_cuda
```


![](asserts/Pasted%20image%2020250812160203.png)

每个核函数一行 
包含函数名、GPU 运行时间、计算吞吐%、内存吞吐% 等信息
%达到最大性能的多少
![](asserts/Pasted%20image%2020250812160227.png)

Details → GPU Speed Of Light Throughput

基础强度0.12

![](asserts/Pasted%20image%2020250812160624.png)

观察： 
◆ 内存吞吐利用率比计算高不少 → 访存密集型 
◆ ncu 会根据测量的指标给出建议

加法的性能瓶颈在内存访问上 
◆ 因此想办法：提升其访存效率 观察： 
◆ 内存使用率虽相对较高，但仍有较大提升空间 
◆ 要增大内存带宽使用，如果一次多传输数据呢？…

### 向量化

◆ 标量操作 → 向量操作 
◆ 向量化（vectorization）：一次同时操作多个标量数据 解决之前的问题： 
◆ 一次多传输数据：向量化访存，即一次同时传输（读写） 多个标量数据 → 提升访存效率
◆ 向量化本质是 SIMD (Single Instruction Multiple Data) 
◆ SIMD: 同时对一组 (向量) 数据中的每一个分别执行相同的操作从而 实现空间上的并行
### SIMT vs SIMD
![](asserts/Pasted%20image%2020250812161157.png)

### 如何向量化
◆ 想进行向量化访存，怎么操作呢？
◆ 多种方式，其中最简单的一种：内置向量化访存类型 
	◆ float2, float3, float4 …
![](asserts/Pasted%20image%2020250812161316.png)

![](asserts/Pasted%20image%2020250812161322.png)
float4 的内部实现

https://docs.nvidia.com/cuda/cuda-c-programming-guide/#built-in-vector-types

### 示例：向量化访存加法

复制add_gpu02创建新文件add_gpu03，做以下修改
```c++
// 设备端模板加法函数，支持标量和 CUDA 向量类型

template <typename T>

__device__ T add(const T &a, const T &b)

{

    if constexpr (std::is_arithmetic_v<T>) {

        // 标量类型（如 float、int 等）直接相加

        return a + b;

    }

    else if constexpr (std::is_same_v<T, float2>) {

        // CUDA 内置二维向量逐分量相加

        return make_float2(a.x + b.x, a.y + b.y);

    }

    else if constexpr (std::is_same_v<T, float4>) {

        // CUDA 内置四维向量逐分量相加

        return make_float4(a.x + b.x, a.y + b.y, a.z + b.z, a.w + b.w);

    }

    else {

        // 不支持的类型触发编译错误

        static_assert(sizeof(T) == 0, "Unsupported type for add()");

    }

}
```
核函数
```c++
// Kernel

template<typename T>

__global__ void add_kernel(T *c, const T *a, const T *b, size_t n, size_t step) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x+step;

    for(size_t i =idx;i<n;i+=step){

        c[i] = add(a[i],b[i]);

    }

}
```
◆ 和之前一样的核函数和外层调用函数 （template！） 
◆ 使用一个模板 add() 函数来封装不同类 型的加法操作 
◆ __device__: 表示这是个仅在 GPU 上 运行的函数 
◆ make_float2, make_float4: float2 和 float4 的创建函数

测试
```
nvcc add_gpu/add_gpu03.cu -o target/add_gpu03
```
[5 特化版本](chatgpt/5%20特化版本.md)

![](asserts/Pasted%20image%2020250812162825.png)

![](asserts/Pasted%20image%2020250812163132.png)
◆ float->float2->float4 计算强度不变，性能提升 → 符合逻辑 ◆ float3 左移了一点，为什么呢？

观察： float->float2->float4 
◆ Memory% 上升但 Compute% 下降 → 访存密集型特征 
◆ 向量化访存倍数增加，Memory% 增加 → 符合逻辑

![](asserts/Pasted%20image%2020250812164336.png)
◆ float2 为什么比 float4 快一点？结论 恒定吗？ 
答案： ◆ 并不是！
◆ 数据量x16，float4 最快了 
疑问： 
◆ 那一直提升向量化倍数能继续提高吗？

### 向量化倍数可以一直提高

![](asserts/Pasted%20image%2020250812164514.png)

观察： 
◆ 增加向量化倍数 → 增加使用的 register 数量 
◆ 但 register 数量有限，超出时会资源紧张/竞争，造成 寄存器溢出（register spilling） 
◆ Register → Local memory，访问延时增加
![](asserts/Pasted%20image%2020250812164636.png)

◆ 但 float3 怎么比 float 还慢…? ◆ ncu 看看！
![](asserts/Pasted%20image%2020250812164740.png)
![](asserts/Pasted%20image%2020250812164756.png)

观察： 
◆ float、float2、float4 ALU（基础计算）的使用均高于 LSU(数据读写)，但 float3 反常明显高于 ALU 位居第一 
◆ …什么意思？
意思是 float3 反常的有更多的读写操作！

◆ 那…为什么呢？
### 内存对齐
◆ GPU 全局内存 (DRAM) 访问以内存事务 (transaction) 为单位（总线宽度决定） 
- → 即便只需1字节数据，也必须搬运整个数据块 
◆ DRAM 的基础事务单位是 32 bytes，缓存行 128 bytes 
◆ 内存对齐：事务的起始地址需是 32 的整数倍 
◆ 如果没有对齐会怎么样？
![](asserts/Pasted%20image%2020250812165233.png)
没办法在一次取出来，时间翻倍

### 内存合并
◆ **CUDA 线程调度的最小单位为 线程束 (warp) (不是每个线程单独调度哦）** 
	→ CUDA 一个 warp 是 32 个线程 
◆ 什么意思？意思就是同一个周期内，一个 warp 中的线程统一执行同一条指令 
◆ 内存合并 (Memory Coalescing)：将同一 warp 的多个内存访问合并为少数事务，最大化内 存带宽利用率

![](asserts/Pasted%20image%2020250812165417.png)
[6 线程束](chatgpt/6%20线程束.md)
◆ 可以看出，要实现内存合并需要的条件： 
- Warp 中的线程需**访问连续的内存地址** 
- 访问的起始地址必须满足**架构对齐**要求  一般是32，缓存行是128
- 限定：同 warp
 
  ◆ 内存合并十分重要！→

![](asserts/Pasted%20image%2020250812200913.png)

下面是比较
- 对齐连续
- 连续不对齐
- 不对齐不连续

疑问： 
- 但 float3 怎么比 float 还慢…? 
- 答案： 
	- 因为直接用 float3 不能很好的对齐 内存！ 
	- float3 = 12 bytes，不是2的幂次 → 无法合并！ （思考：有办法修复吗？…）
![](asserts/Pasted%20image%2020250812201203.png)


### 向量化访存性能提高

◆ 使用向量化访存确实会提升原本访存受限的并行加法性能 
◆ 分析出来的数据证明了这一点，但… 
◆ 有没有更直接的方法能看出向量化访存提升性能的原理呢？ 
◆ 当然有！

### 前情回顾：编译流程
◆ 设备端编译流程中间每个**虚拟架构**会编 译出一个 .ptx 的文件 
◆ 这里面包含着该文件对该虚拟架构的 PTX (Parallel Thread Execution) 代码 
◆ 是一种低级的并行线程执行的虚拟指令 集（ISA）
![](asserts/Pasted%20image%2020250812201436.png)

- https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#compilation-phases

### 向量化访存-浅看PTX

![](asserts/Pasted%20image%2020250812201812.png)

◆ 6 个 fp32 load 
◆ 3 个 fp32 add 
◆ 3 个 fp32 store 
◆ **float3 没能合并访存 → 单纯 3 个 float**

### 性能能否进一步提升？
![](asserts/Pasted%20image%2020250812202018.png)
◆ 右移 → 提高 AI 
	◆ 𝐴𝐼 = 𝑊/𝑄 
	1. 提高 W； → 增加计算量 → 算子融合 
	2. 降低 Q； →减少访存压力 → 如果传更少数据
	3. 提高 W + 降低 Q；

### 传更少的精度
◆ 减少访存压力 → 如果同样的计算能传更少的数据不就能降低 Q 从而提高 AI 了吗？ ◆ float/float32 = 32 bits，如果减半… 
◆ half/float16！

![](asserts/Pasted%20image%2020250812202318.png)

### 示例：半精度加法
◆ 和之前一样的核函数、外层调用函数和封 装的 add() 函数（template！） 
◆ 需要 include 以使用 CUDA 关于 fp16/half 的相关支持 
◆ half: CUDA 支持的内置半精度类型
◆ __hadd(): 进行两个半精度数加法的内置 函数 
◆ __float2half()/__half2float()：half 与 float 之间的转换函数

### 舍入模式
◆ 一些类型转换函数会有不同的舍入模式版本 
◆ .rd, .ru, .rn, .rz 
◆ RD：Round Down, Round Toward Negative Infinity. E.g., -2.5 → -3.0 
◆ RU：Round Up, Round Toward Positive Infinity. E.g., -2.5 → -2.0 
◆ RN: Round-to-Nearest-Even, 四舍五入，但中间时会舍入到最近的偶数. E.g., -2.5 → -2.0 
◆ RZ: Round Toward Zero，向零方向舍入. E.g., -2.5 → -2.0 
◆ IEEE 754 默认 RN

### 使用半精度 (half)

![](asserts/Pasted%20image%2020250812202805.png)
增大数据量：
观察： 
◆ ？！half 快了好多！（当然…） 
◆ 但还能不能更快？ 
◆ 能！ 
◆ float 能向量化访存，half 也行！
![](asserts/Pasted%20image%2020250812202940.png)
### 半精度并行加法

◆ 和之前一样的核函数、外层调用函数 和封装的 add() 函数（template！） 
◆ half2: CUDA 支持的内置半精度类型 
◆ 用 float 之类的也可以

![](asserts/Pasted%20image%2020250812203326.png)

half2的PTX
![](asserts/Pasted%20image%2020250812203351.png)
◆ 好像有点复杂 
◆ 不过因为我们把 half2 的每部分都拆开来对位计算，载入没有存回的向量化程度 
◆ 而且用了两个加法指令 
◆ 有没有办法优化？

### 向量化计算
![](asserts/Pasted%20image%2020250812203622.png)


![](asserts/Pasted%20image%2020250812203711.png)

![](asserts/Pasted%20image%2020250812203726.png)


![](asserts/Pasted%20image%2020250812203747.png)



float → half 增幅大的原因： 
◆ AI 右移给出更多性能提升空间 
◆ 移动的幅度理论上是翻倍 → 提升不小 
◆ 回忆：Roofline 是跟硬件相关的！ 
	→ 天花板会变！FP16 比 FP32 的算力更高！ 
◆ Roofline 中的 kernel point 大概率同时还会上移！ （i.e., 右上移动相对于原 FP32，而非仅仅右移）

![](asserts/Pasted%20image%2020250812203923.png)


![](asserts/Pasted%20image%2020250812204015.png)


◆ Roofline 增加 W 和减少 Q 的其他方法？ 
◆ 新的挑战！ 

◆ 思考： 

1. 可以有效使用 float3 吗？如果可以，怎么做？
2. 之前提到向量化访存有多种方法，今天主要介绍了使用内置类型的方式，想想还有 什么其他方式吗？（Hint: 今天的课中其实已经提到了，同时也和第一问有关）