![](asserts/Pasted%20image%2020250812150817.png)

![](asserts/Pasted%20image%2020250812150847.png)

![](asserts/Pasted%20image%2020250812150831.png)


![](asserts/Pasted%20image%2020250812150915.png)

- https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf

- GPU内存带宽


### 内存墙
◆ 摩尔定律：算力每18-24个月翻倍 
◆ 但 DRAM 每年 +~10% 
◆ 冯·诺依曼架构: **数据必须在处理器和内存之间频繁传输** 
◆ 数据量大或计算密度低时 → 内存瓶颈 
◆ 总结：处理器（CPU/GPU）计算速度远快于 内存访问速度！

![](asserts/Pasted%20image%2020250812151432.png)
https://arxiv.org/pdf/2403.14123

观察： 
◆ 19.5 TFLOPS = 𝟏. 𝟗𝟓 × 𝟏𝟎𝟏𝟑 𝒐𝒑/𝒔𝒆𝒄 
◆ 2039 GB/s = 𝟐. 𝟎𝟑𝟗 × 𝟏𝟎𝟏𝟐 𝒃𝒚𝒕𝒆𝒔/𝒔𝒆𝒄 
◆ 即便 80GB HBM2e，速度相对算力有限 
◆ 对于 float 加法而言： 
	◆ 12 bytes 内存访问（2读+1写） 
	◆ 1 op 
◆ → **本身需要的内存访问较多，而计算量很少**

◆ 重要发现：理论上加法的性能瓶颈在内存访问上 
◆ 如何直观快速的判断一个算子在某个硬件上的性能瓶颈？ 
◆ 如何量化性能瓶颈？
→ 有助于判断和优化

### Roofline模型
![](asserts/Pasted%20image%2020250812151909.png)

![](asserts/Pasted%20image%2020250812152011.png)
β最大内存带宽
I 最大计算强度

特性： 
◆ 硬件有影响——Hardware-dependent 

◆ 拐点：也叫脊点（ridge point），可达到该硬 件理论最高性能的最低计算强度 


◆ 脊点以左：访存密集型，瓶颈在访存 
◆ 脊点以右：计算密集型，瓶颈在计算

示例： 
◆ Float 加法: 1 𝐹𝐿𝑂𝑃 /12 𝑏𝑦𝑡𝑒𝑠 = 1 /12 ≈ 𝟎. 𝟎𝟖𝟑 𝑭𝑳𝑶𝑷 𝒃𝒚𝒕𝒆 
◆ A100： 
◆ 19.5 TFLOPS = 𝟏. 𝟗𝟓 × 𝟏𝟎𝟏𝟑 𝒐𝒑/𝒔𝒆𝒄 
◆ 2039 GB/s = 𝟐. 𝟎𝟑𝟗 × 𝟏𝟎𝟏𝟐 𝒃𝒚𝒕𝒆𝒔/𝒔𝒆𝒄 
◆ 𝐼𝑚𝑎𝑥 = 𝟏.𝟗𝟓×𝟏𝟎𝟏𝟑 𝑭𝑳𝑶𝑷/𝒔𝒆𝒄 𝟐.𝟎𝟑𝟗×𝟏𝟎𝟏𝟐 𝒃𝒚𝒕𝒆𝒔/s𝒆𝒄 ≈ 𝟗. 𝟓𝟔 𝑭𝑳𝑶𝑷 𝒃𝒚𝒕𝒆 

◆ 确实是访存密集型
◆ 加法是比较典型的访存密集型算子 
### 优化访存密集型很重要
◆ 计算密集型？→ GEMM，之后… 
◆ **访存密集型算子占大多数**，因为 
	◆ 内存墙 ：内存硬件和计算机架构导致算子很容易受到内存影响
	◆ 深度学习等领域数据量大 
◆ 因此，**优化访存密集型算子**很重要！ 

◆ 刚才的 Roofline 分析提供**理论基础**，实际情况更为复杂，实际达到的性能还取决于其他因 素…（e.g., 代码实现）
- 算子本身不够效率
- 时钟频率？


### Nsight Compute
使用完整路径名
```
 sudo /usr/local/cuda/bin/ncu --print-details all --nvtx --call-stack --set full ./target/add_gpu02
```
[3 修改性能计数器权限](chatgpt/3%20修改性能计数器权限.md)
- WSL2 支持 GPU 加速（NVIDIA CUDA），但很多硬件层面的调试和性能分析功能受限。
- Nsight Compute 在 WSL 里只能做最基本的分析，不能访问底层硬件计数器。
- 这就是你看到 `ERR_NVGPUCTRPERM` 的根本原因，无法解决。

◆ 想分析之前的 GPU 并行加法代码 
◆ → Nsight Compute (ncu) ：英伟达的内核级性能分析工具 
◆ 使用 ncu CLI ：
```
sudo ncu --print-details --nvtx --call-stack --set full ./add_cuda
```
◆ 导出文件并使用 ncu GUI：
```
sudo ${ncu} --nvtx --call-stack --set full -f --export add_cuda.ncu-rep ./add_cuda
```


![](asserts/Pasted%20image%2020250812160203.png)

每个核函数一行 
包含函数名、GPU 运行时间、计算吞吐%、内存吞吐% 等信息
%达到最大性能的多少
![](asserts/Pasted%20image%2020250812160227.png)

Details → GPU Speed Of Light Throughput

基础强度0.12

![](asserts/Pasted%20image%2020250812160624.png)

观察： 
◆ 内存吞吐利用率比计算高不少 → 访存密集型 
◆ ncu 会根据测量的指标给出建议

加法的性能瓶颈在内存访问上 
◆ 因此想办法：提升其访存效率 观察： 
◆ 内存使用率虽相对较高，但仍有较大提升空间 
◆ 要增大内存带宽使用，如果一次多传输数据呢？…

### 向量化

◆ 标量操作 → 向量操作 
◆ 向量化（vectorization）：一次同时操作多个标量数据 解决之前的问题： 
◆ 一次多传输数据：向量化访存，即一次同时传输（读写） 多个标量数据 → 提升访存效率
◆ 向量化本质是 SIMD (Single Instruction Multiple Data) 
◆ SIMD: 同时对一组 (向量) 数据中的每一个分别执行相同的操作从而 实现空间上的并行
### SIMT vs SIMD
![](asserts/Pasted%20image%2020250812161157.png)

