## 1\. **为什么单看 FLOP 不够**

-   **FLOP（Floating Point Operations）**：只是计算量的总和，例如 Transformer 里矩阵乘法、注意力机制等的乘加次数。
    
-   但是 **性能瓶颈** 不一定在计算单元，也可能在**内存带宽**。
    
-   如果模型每次计算都需要频繁从显存加载大量数据（权重、激活值等），那么即使算力很强，也会因为等待数据而变慢——这叫**内存带宽受限**（Memory Bound）。
    

---

## 2\. **算术强度（Arithmetic Intensity, AI）定义**

$$
\text{AI} = \frac{\text{FLOP 总数}}{\text{内存访问字节数（MOP）}}
$$

-   **分子**：总计算量（单位：FLOP）
    
-   **分母**：总的内存访问量（单位：Byte）
    
-   也就是说：
    
    -   **AI 高** → 每加载一个字节，可以做很多计算 → 更可能是 **计算受限（Compute Bound）**
        
    -   **AI 低** → 每加载一个字节，计算很少 → 更可能是 **内存受限（Memory Bound）**
        

---

## 3\. **例子：Transformer 编码器 vs 解码器**

假设：

-   编码器的矩阵乘法占比高，权重重复利用率高 → **算术强度高**
    
-   解码器在自回归生成时一次只处理少量 token，且需要频繁访问历史缓存 → **算术强度低**
    

**结果：**

-   编码器可能更容易受计算性能限制（GPU 算力瓶颈）
    
-   解码器更可能受限于显存带宽（尤其是长序列推理）
    

---

## 4\. **Roofline 模型**

为了更直观评估性能瓶颈，可以用 **Roofline Model**：

-   横轴：AI（FLOP/Byte）
    
-   纵轴：性能（GFLOP/s）
    
-   两条限制线：
    
    -   **带宽线**：受内存带宽限制的上限
        
    -   **算力线**：受计算峰值限制的上限
        
-   模型位置：
    
    -   **左边区域** → 内存受限
        
    -   **右边区域** → 计算受限
        

---

## 5\. **总结**

-   **单看 FLOP 容易错判瓶颈**，因为它没考虑内存访问开销。
    
-   **算术强度 AI** 可以揭示运算和数据访问的比例，帮助判断优化方向。
    
-   **Transformer 优化策略**：
    
    -   提高权重重用率（减少内存访问）
        
    -   增加批处理或并行度（提升 AI）
        
    -   内存布局优化，提升 Memory Coalescing 效果
        

---