
![](asserts/Pasted%20image%2020250813155747.png)
L1cache share memory
共享内存核L1缓存是公用同一个物理空间的，每一行是32个bank，一个bank是4bytes
### Bank Conflict
◆ 共享内存被划分为 32 个 Bank，每个 bank 是 4 bytes 
◆ 回顾：一个 warp 有 32 个线程 
◆ 并非巧合！**连续地址的数据按 Bank 大小循环映射到不同 Bank** 
◆ **同一个 warp 的所有线程同时执行同一无冲突的指令，比如 ld/sw 4 bytes，1 cycle 完成**
![](asserts/Pasted%20image%2020250813155853.png)


◆ 但当一个 warp 中的**多个线程同时（Instr./TX）访问同一个 bank 的不同地址**时，这时 候访问将会被串行化，这个现象就叫 Bank Conflict。

重点：访问规律/模式 

◆ 如果多个线程同时访**问同一个 bank 的同一个地址**时，这时不会发生 bank conflict，而 是会直接发生广播 (Broadcast) 或多播 (Multicast)
- 一个循环完成

◆ 因此，Bank conflict 常发生于**不连续访问共享内存的情况** 
◆ 最常见的不连续访问情景就是跨步访问（比如刚才的树状规约） 
◆ 当然还有其他较为少见的情况，比如随机访问 
◆ 来分析一下刚才的情况

### 不连续访存案例

第一层规约：
```
T0: smem[0] 和 smem[1] → Bank 0, Bank1 
T1: smem[2] 和 smem[3] → Bank 1, Bank2
… 
T16: smem[32] 和 smem[33] → Bank 0, Bank1 ! 

Bank Conflict! (2-way)
```
每个地方会有两个线程冲突。

![](asserts/Pasted%20image%2020250820161130.png)


### 优化方法
◆ 好吧… 那 Bank conflict 怎么解决呢？ 
◆ 有多种办法，其中第一种方式是: 
1. 算法/访存模式优化
2. 内存填充

◆ 这个方法是遇到 bank conflict 的上上策，因为是最彻底、最根源性的解决方法 
◆ 比如刚才那个例子，我们就可以使用之前的对半向前的树状规约模式


第一层规约：
```
T0: smem[0] 和 smem[32] → Bank 0, Bank0 
T1: smem[1] 和 smem[33] → Bank 1, Bank1 … 
T31: smem[31] 和 smem[63] → Bank 31, Bank31
```

不同线程访问不同地址才是冲突
### NCU bank conflict对比
![](asserts/Pasted%20image%2020250813160358.png)
- L1 Wave 每一行代码有没有
- Memory Workload Analysis 中是有数值的

◆ 那有时候算法上没法很好的替换和解决，怎么办？
1. 算法/访存模式优化 
2. 内存填充（Padding） 
3. ? ? ?
### 内存填充（Padding） 
◆ 看一个例子，transpose（转置）核函 数

```cpp
#define BLOCK_DIM 16
__global__ void transpose_kernel(float* output, const float* input, int matrix_dim) {
    __shared__ float tile[BLOCK_DIM][BLOCK_DIM];
    // `blockIdx.{x, y, z}`：Block 在 Grid 内的三维坐标
    // `threadIdx.{x, y, z}`：线程在 Block 内的三维坐标
    int col = blockIdx.x * BLOCK_DIM + threadIdx.x;
	int row = blockIdx.y * BLOCK_DIM + threadIdx.y;
	// `atrix_dim` 一般是 **矩阵的维度大小**，也就是矩阵的边长。
    if (row < matrix_dim && col < matrix_dim) {
	    //**读入阶段**：每个线程按照行方向连续读（访问合并，快）。
        tile[threadIdx.y][threadIdx.x] = input[row * matrix_dim + col];
    }
    __syncthreads();
	
    int transposed_col = blockIdx.y * BLOCK_DIM + threadIdx.x;
    int transposed_row = blockIdx.x * BLOCK_DIM + threadIdx.y;

    if (transposed_row < matrix_dim && transposed_col < matrix_dim) {
	    // **写出阶段**：线程在共享内存里做转置，再写出去（依然合并）。
        output[transposed_row * matrix_dim + transposed_col] = tile[threadIdx.x][threadIdx.y];
    }
}
```






◆ 该实现是否有 Bank conflict？ 
◆ 有！
◆ 最后一行，进行存入的时候 
◆ 几路冲突？
### 消除冲突


![](asserts/Pasted%20image%2020250813160635.png)


◆ 为什么这样就没有 Bank conflict 了？？ 
◆` tile[threadIdx.x][threadIdx.y]`
◆ 2D 结构时会隐式计算线性索引： linear_idx = threadIdx.y * blockDim.x + threadIdx.x; 
◆ y → 行，外层；
x → 列，内层 
◆ 同一 threadIdx.y 的时候，threadIdx.x 全部在访问同一个 Bank！
![](asserts/Pasted%20image%2020250821094635.png)

![](asserts/Pasted%20image%2020250813160727.png)

◆ 填充之后的逻辑视图相当于变成了 这样： 
◆ Bank 不再是一条竖线 
◆ 以 Bank 0 为例，填充后变成这样：
![](asserts/Pasted%20image%2020250813160804.png)
◆ **回归到物理视图，warp 的 访问就变成这样**：
◆ 从这两个视图可以看出： 
◆ 确实没有 Bank conflict 了！ 
◆ 每个 warp 内都在访问不 同的 Bank！ 
◆ 好，我们看看 ncu 确认

![](asserts/Pasted%20image%2020250813161054.png)
![](asserts/Pasted%20image%2020250813164224.png)


◆ 嗯，ncu 显示这一 行确实有 bank conflict！ 
◆ L1 wavefront shared excessive 不为 0！


![](asserts/Pasted%20image%2020250813164325.png)

◆ 嗯？？ncu 在 source 里 的 L1 Wavefront Shared Excessive 显示为 0 
◆ 但下面 Details 里显示有 bank conflict！？
◆ 这是怎么回事，难道我们 分析错了？

### 访问模式上的bankconflict

◆ 共享内存与 L1 缓存共享 物理空间！

访问共享内存，L1缓存可能被其他内存所访问，共享内存和L1缓存的冲突，不属于我们能操控的，访问模式上的bankconflict，所以L1 Wavefront Shared Excessive 显示为 0 ，detail显示有
### Padding
◆ 懂了懂了，但有一个问题： 
◆ Padding 会额外消耗 smem 的空间，根据情景可能会浪费许多宝贵的 smem 空间 
◆ 有没有缓解方法？ 
1. 算法/访存模式优化 
2. 内存填充（Padding）
3. ? ? ?

◆ 解决 Bank conflict 的另一优雅技巧！ 
◆ AI 核心——矩阵乘法! 
◆ 思考：
1. Padding 可以解决所有的 bank conflict 情况吗？如果不能，有什么例子吗？（Hint）
2. 还有什么方法可以缓解或消除 bank conflict？
3. 共享内存和 L1 缓存共享物理空间，那相同的访问模式/规律是不是访问他们都是一样的 效率/性能？试试看

![](asserts/Pasted%20image%2020250820163047.png)