### 示例：向量化访存加法

复制add_gpu01创建新文件add_gpu02_f4，做以下修改
```c++
// 设备端模板加法函数，支持标量和 CUDA 向量类型

template <typename T>

__device__ T add(const T &a, const T &b)

{

    if constexpr (std::is_arithmetic_v<T>) {

        // 标量类型（如 float、int 等）直接相加

        return a + b;

    }

    else if constexpr (std::is_same_v<T, float2>) {

        // CUDA 内置二维向量逐分量相加

        return make_float2(a.x + b.x, a.y + b.y);

    }

    else if constexpr (std::is_same_v<T, float4>) {

        // CUDA 内置四维向量逐分量相加

        return make_float4(a.x + b.x, a.y + b.y, a.z + b.z, a.w + b.w);

    }

    else {

        // 不支持的类型触发编译错误

        static_assert(sizeof(T) == 0, "Unsupported type for add()");

    }

}
```
核函数
```c++
// Kernel

template<typename T>

__global__ void add_kernel(T *c, const T *a, const T *b, size_t n, size_t step) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x+step;

    for(size_t i =idx;i<n;i+=step){

        c[i] = add(a[i],b[i]);

    }

}
```
◆ 和之前一样的核函数和外层调用函数 （template！） 
◆ 使用一个模板 add() 函数来封装不同类 型的加法操作 
◆ __device__: 表示这是个仅在 GPU 上 运行的函数 
◆ make_float2, make_float4: float2 和 float4 的创建函数

测试
```
nvcc add_gpu/add_gpu03.cu -o target/add_gpu03
```
[5 特化版本](chatgpt/5%20特化版本.md)
### VS提升前
![](asserts/Pasted%20image%2020250812162825.png)

![](asserts/Pasted%20image%2020250812163132.png)
◆ float->float2->float4 计算强度不变，性能提升 → 符合逻辑 ◆ float3 左移了一点，为什么呢？

观察： float->float2->float4 
◆ Memory% 上升但 Compute% 下降 → 访存密集型特征 
◆ 向量化访存倍数增加，Memory% 增加 → 符合逻辑

![](asserts/Pasted%20image%2020250812164336.png)
◆ float2 为什么比 float4 快一点？结论 恒定吗？ 
答案： ◆ 并不是！
◆ 数据量x16，float4 最快了 
疑问： 
◆ 那一直提升向量化倍数能继续提高吗？

### 向量化倍数可以一直提高

![](asserts/Pasted%20image%2020250812164514.png)

观察： 
◆ 增加向量化倍数 → 增加使用的 register 数量 
◆ 但 register 数量有限，超出时会资源紧张/竞争，造成 寄存器溢出（register spilling） 
◆ Register → Local memory，访问延时增加
![](asserts/Pasted%20image%2020250812164636.png)

◆ 但 float3 怎么比 float 还慢…? ◆ ncu 看看！
![](asserts/Pasted%20image%2020250812164740.png)
![](asserts/Pasted%20image%2020250812164756.png)

观察： 
◆ float、float2、float4 ALU（基础计算）的使用均高于 LSU(数据读写)，但 float3 反常明显高于 ALU 位居第一 
◆ …什么意思？
意思是 float3 反常的有更多的读写操作！

◆ 那…为什么呢？
### 内存对齐
◆ GPU 全局内存 (DRAM) 访问以内存事务 (transaction) 为单位（总线宽度决定） 
- → 即便只需1字节数据，也必须搬运整个数据块 
◆ DRAM 的基础事务单位是 32 bytes，缓存行 128 bytes 
◆ 内存对齐：事务的起始地址需是 32 的整数倍 
◆ 如果没有对齐会怎么样？
![](asserts/Pasted%20image%2020250812165233.png)
没办法在一次取出来，时间翻倍

### 内存合并
◆ **CUDA 线程调度的最小单位为 线程束 (warp) (不是每个线程单独调度哦）** 
	→ CUDA 一个 warp 是 32 个线程 
◆ 什么意思？意思就是同一个周期内，一个 warp 中的线程统一执行同一条指令 
◆ 内存合并 (Memory Coalescing)：将同一 warp 的多个内存访问合并为少数事务，最大化内 存带宽利用率

![](asserts/Pasted%20image%2020250812165417.png)
[6 线程束](chatgpt/6%20线程束.md)
◆ 可以看出，要实现内存合并需要的条件： 
- Warp 中的线程需**访问连续的内存地址** 
- 访问的起始地址必须满足**架构对齐**要求  一般是32，缓存行是128
- 限定：同 warp
 
  ◆ 内存合并十分重要！→

![](asserts/Pasted%20image%2020250812200913.png)

下面是比较
- 对齐连续
- 连续不对齐
- 不对齐不连续

疑问： 
- 但 float3 怎么比 float 还慢…? 
- 答案： 
	- 因为直接用 float3 不能很好的**对齐内存**！ 
	- float3 = 12 bytes，不是2的幂次 → 无法合并！ （思考：有办法修复吗？…）
![](asserts/Pasted%20image%2020250812201203.png)
### 总结
### 1. 计算次数提升（利用 SIMD 并行）
- 一条指令处理多个数据元素（比如 4 个 float），相较标量操作，单位时间内完成更多计算
- 大幅提升算力利用率，减少循环次数和指令调度开
### 2. 传输次数减少（内存事务合并）
- 向量化数据通常是连续存储的，能够以较大块（例如 16、32、64 字节）为单位读取
- **减少了单个数据的访问次数**，一次访存带来更多有效数据
- 内存控制器和缓存更好地利用宽总线和缓存行，实现内存合并（memory coalescing）