数据大小：const size_t SIZE = 1 << 24; // 元素总数 half个数

```
nvcc add_gpu/add_gpu03_half.cu -o target/add_gpu03_half
```

```
nsys profile -t cuda,nvtx,osrt -o target/add_gpu03 -f true target/add_gpu03_half
```

```
nsys stats target/add_gpu03.nsys-rep
```

```
./target/add_gpu03
```

```
 ** CUDA API Summary (cuda_api_sum):

 Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)           Name         
 --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------
     71.2       1157435798          4  289358949.5  295968651.5  251330629  314167866   28213318.3  cudaMemcpy            
     25.0        405696811          3  135232270.3  133411234.0   66925750  205359827   69235002.3  cudaMalloc            
      3.5         57224137          3   19074712.3   17534783.0   16559163   23130191    3545862.2  cudaFree              
      0.2          3650248          1    3650248.0    3650248.0    3650248    3650248          0.0  cuLibraryLoadData     
      0.1           953776          1     953776.0     953776.0     953776     953776          0.0  cudaLaunchKernel      
      0.0             1431          1       1431.0       1431.0       1431       1431          0.0  cuModuleGetLoadingMode
      0.0             1234          1       1234.0       1234.0       1234       1234          0.0  cuLibraryGetKernel    
      0.0              502          1        502.0        502.0        502        502          0.0  cuKernelGetName       
      
```
### 传更少的精度
◆ 减少访存压力 → 如果同样的计算能传更少的数据不就能降低 Q 从而提高 AI 了吗？ ◆ float/float32 = 32 bits，如果减半… 
◆ half/float16！

![](asserts/Pasted%20image%2020250812202318.png)

### 示例：半精度加法
◆ 和之前一样的核函数、外层调用函数和封 装的 add() 函数（template！） 
◆ 需要 include 以使用 CUDA 关于 fp16/half 的相关支持 
◆ half: CUDA 支持的内置半精度类型
◆ __hadd(): 进行两个半精度数加法的内置 函数 
◆ __float2half()/__half2float()：half 与 float 之间的转换函数

### 舍入模式
◆ 一些类型转换函数会有不同的舍入模式版本 
◆ .rd, .ru, .rn, .rz 
◆ RD：Round Down, Round Toward Negative Infinity. E.g., -2.5 → -3.0 
◆ RU：Round Up, Round Toward Positive Infinity. E.g., -2.5 → -2.0 
◆ RN: Round-to-Nearest-Even, 四舍五入，但中间时会舍入到最近的偶数. E.g., -2.5 → -2.0 
◆ RZ: Round Toward Zero，向零方向舍入. E.g., -2.5 → -2.0 
◆ IEEE 754 默认 RN

### 使用半精度 (half)

![](asserts/Pasted%20image%2020250812202805.png)
增大数据量：
观察： 
◆ ？！half 快了好多！（当然…） 
◆ 但还能不能更快？ 
◆ 能！ 
◆ float 能向量化访存，half 也行！
![](asserts/Pasted%20image%2020250812202940.png)
### 总结
减少传输数据大小