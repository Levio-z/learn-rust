
◆ Roofline 增加 W 和减少 Q 的其他方法？ 
◆ 新的挑战！ 

◆ 思考： 

1. 可以有效使用 float3 吗？如果可以，怎么做？
	- 内存对齐，填充数据:尽量确保它们按 16 字节对齐（使用对齐属性 `__align__` 或显式填充）

2. 之前提到向量化访存有多种方法，今天主要介绍了使用内置类型的方式，想想还有 什么其他方式吗？（Hint: 今天的课中其实已经提到了，同时也和第一问有关）
	- “结构体分离”或“结构体拆分”
		- 将每个分量独立存储为连续数组，比如三个 `float` 数组分别存储 `x`、`y`、`z` 分量
		- 这样能保证每个数组都是紧凑连续的，方便 warp 内存合并，提高带宽利用
	- 数据打包（Packing）和手动对齐
		-  通过显式对齐和填充，将不对齐的结构转化为对齐的向量类型（如用 `float4` 替代 `float3`）
		- 避免跨事务访问，提高访存效率
	- 使用 `cudaMemcpy2D` 或类似的批量拷贝操作
		- 利用 CUDA API 提供的多维数据拷贝，提升访存效率
	- 自定义向量化数据类型
		- 设计自定义的对齐和打包数据结构，利用 `__align__` 和内存屏障等机制，精细控制访存模式
