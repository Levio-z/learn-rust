sum_gpu_03_reduce_tree.cu
### 测试
数据大小：const size_t SIZE = 1 << 20; // 元素总数 half个数
```
nvcc sum_gpu/sum_gpu_03_reduce_tree.cu -o target/sum_gpu_03_reduce_tree

nsys profile -t cuda,nvtx,osrt -o target/sum_gpu_03_reduce_tree -f true target/sum_gpu_03_reduce_tree

nsys stats target/sum_gpu_03_reduce_tree.nsys-rep

./target/sum_gpu_03_reduce_tree

```



### 加大分治粒度
问题：是否这就是最优？如果不是，如何提升？ 
◆ Compute % > memory %，而且需要提高利用率 
◆ **分治规约…那能不能加大分治粒度？** 
◆ 可以，但线程怎么做到 warp/block 内共享/通信？

问题： 
◆ 加大分治粒度，但线程怎么做到 warp/block 内共享/通信？ 
答案： 
◆ 可以用共享内存！ 
◆ 数据在共享内存中对同块中的线程均可见

### 树状规约
◆ 利用共享内存做树状规约 
◆ 树状规约的方式貌似有些…不习惯? 
◆ 这里使用动态分配共享内存 
◆ 核函数发射第三个参数加入动态分配的共享 内存大小 

```c++
template <typename T>

__global__ void reduce_smem_tree_kernel(T *output, const T *input, size_t n) {

    // 共享内存声明（外部共享内存，编译时由核函数参数指定大小）

    extern __shared__ T smem[];  

    size_t tid = threadIdx.x;            

    size_t idx = blockIdx.x * blockDim.x + tid;  

  

    // 1. 加载全局内存到共享内存

    smem[tid] = (idx < n) ? input[idx] : 0;

    __syncthreads();  // 确保所有线程完成加载

  

    // 2. 共享内存树状归约

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {

        if (tid < s) {

            smem[tid] += smem[tid + s];

        }

        __syncthreads();  // 同步确保阶段内结果正确

    }

  

    // 3. 单个 Block 归约完成后，用原子操作合并到全局 output

    if (tid == 0) {

        atomicAdd(output, smem[0]);

    }

}
```


◆ __shared__: 表示该变量分配至共享内存 
◆ __syncthreads(): 同步块内线程
- 保证上面的线程都进行了操作，同步函数

1. **第一轮**：线程 0 与线程 4 合并（假设 blockSize=8），线程 1 与线程 5 合并……（所有满足 `tid < s` 的线程参与）
2. **第二轮**：线程 0 与线程 2 合并，线程 1 与线程 3 合并……
3. **最后一轮**：仅线程 0 参与，与线程 1 合并，最终结果写入 `smem[0]`可见，`smem[0]` 的结果是**多轮并行计算的累积**，每个线程在不同阶段都参与了数据合并，只是最后一轮由线程 0 完成最终合并。

![](asserts/Pasted%20image%2020250820110331.png)
