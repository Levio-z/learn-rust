**1\. 推理背景**

-   **完成一次计算** = **算术执行时间** + **数据传输时间**。
-   数据来源：
    1.  **缓存命中**（latency 很低）
    2.  **缓存未命中 → DRAM**（latency 很高）
>latency:延迟
---

**2\. 例子假设**

- **80% 的数据命中缓存**
    - 意味着这部分数据直接从 CPU 缓存（L1/L2/L3）读取
    - 延迟很低，可能只要 1–10 个 CPU 周期（cycle
- **20% 的数据必须去 DRAM**
    - 说明缓存里没有这些数据（缓存未命中，Cache Miss）
    - 必须访问主存（DRAM）才能拿到
    - DRAM 延迟远高于缓存

- 这里的 “> 5 个周期” 是一个**临界值假设**
    - 如果 **DRAM 延迟** 超过这个值
    - 那么即便只有 20% 的数据要去 DRAM，整体执行时间也会被这部分高延迟拖慢到接近 DRAM 的速度
⚠️ 注意：现实中的 DRAM 延迟**远大于** 5 个 cycle，通常是 **100–200+ cycle**，所以在真实 CPU 中这个瓶颈更严重。
>“即使 DRAM 只比缓存慢一点点（超过 5 个周期），整体性能也会变成 DRAM 说了算。”
---

**3\. 推理核心**  
即便 80% 的数据很快得到，剩下 20% 的数据访问成本依然可能**完全淹没**计算速度。  
当 DRAM 延迟超过一个临界点（例子中是 5 个周期）时，**总完成时间由 DRAM 决定**——哪怕 CPU 的算术单元变得更快也无济于事。

---

**4\. 结论：内存墙**

-   CPU 性能增长速度（算术能力提升）  
    ≫
-   内存访问性能提升速度（尤其是 DRAM 延迟改善）
    

这种不平衡意味着，随着时间推移，**瓶颈越来越集中在内存子系统**。  
最终出现“内存墙”——**CPU 在等待数据，无法充分利用算术执行单元**。