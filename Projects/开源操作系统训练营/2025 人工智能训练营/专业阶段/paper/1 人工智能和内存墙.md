要 — **前所未有的无监督训练数据的可用性以及神经缩放定律导致服务/训练 LLM 的模型大小和计算需求空前激增**。然而，主要的性能瓶颈越来越多地转移到内存带宽上。在过去的 20 年里，服务器硬件 FLOPS 的峰值一直以 3.0×/2 年的速度扩展，超过了 DRAM 和互连带宽的增长速度，后者分别仅以每 2 年的 1.6 倍和 1.4 倍的速度扩展。这种差异使内存而不是计算成为人工智能应用程序的主要瓶颈，尤其是在服务方面。在这里，我们分析了编码器和解码器 Transformer 模型，并展示了内存带宽如何成为解码器模型的主要瓶颈。我们主张重新设计模型架构、训练和部署策略，以克服这种内存限制。

### 一、介绍
训练大型语言模型 （LLM） 所需的计算量最近以 750×/2 年的速度增长。这种指数趋势一直是人工智能加速器的主要驱动力，这些加速器专注于提高硬件的峰值计算能力，通常以简化内存层次结构等其他部分为代价。

然而，这些趋势忽略了训练和服务人工智能模型的一个新挑战：内存和通信瓶颈。事实上，一些人工智能应用正因芯片内/芯片间以及跨人工智能加速器/与人工智能加速器之间的通信而成为瓶颈，而不是计算。这并不是一个新现象，过去的几部作品都对这个问题进行了观察和警告。对此最早的观察可以追溯到 1990 年，当时 Ousterhout 在分析了影响作系统性能的因素后得出以下结论 [30]：

>“第一个与硬件相关的问题是内存带宽：基准测试表明它跟不上 CPU 速度......如果未来机器的内存带宽没有显着提高，某些类别的应用程序可能会受到内存性能的限制。

![](asserts/Pasted%20image%2020250812211242.png)
图 1：不同代互连和内存的带宽缩放，以及峰值 FLOPS。可以看出，带宽的增长非常缓慢。我们正在使用 R10000 系统对硬件峰值 FLOPS 进行归一化，因为它被用来报告训练 LeNet-5 的成本[22]。

1995 年晚些时候，威廉·伍尔夫 （William Wulf） 和莎莉·麦基 （Sally Mckee） 进一步呼应了这一预测，并创造了“内存墙”一词。他们对此的论点遵循了一个简单而优雅的推理。**完成运算的时间取决于我们执行算术的速度以及我们将数据馈送到硬件算术单元的速度**。在最简单的情况下，数据要么在缓存中可用，或需要从 DRAM 中获取。有了这个假设，即使 80% 的数据在缓存中随时可用，并且只需要从 DRAM 中获取 20% 的数据，如果需要超过 5 个周期才能从 DRAM 中获取 20% 的缓存未命中数据，则完成作的时间将完全受到 DRAM 的限制。这意味着无论硬件每秒执行算术运算的速度有多快，问题都将完全受到 DRAM 带宽的限制。他们预测，计算速度与数据获取速度的改进速度不同，将产生“内存墙”问题[25,42]。基于此，他们得出结论：

>“每一种都在呈指数级增长，但微处理器的指数比 DRAM 的指数大得多。发散指数之间的差异也呈指数级增长。

后来的几篇工作也报告了类似的观察结果[12,24,25,31,36,40]。

在这项工作中，我们通过研究最近的数据重新审视了这一趋势，特别关注用于训练人工智能模型的硬件，以及用于训练/服务它们的计算特征。30 年后，上述观察和预测再也无法正确。尽管内存技术出现了许多创新，但趋势表明，“内存墙”正日益成为一系列人工智能任务的主要瓶颈。


我们首先分析服务器的峰值计算
![](asserts/Pasted%20image%2020250812211403.png)
图 2：（a）多年来最先进的（SOTA）模型参数数量的演变，以及 AI 加速器内存容量（绿点）。大型 Transformer 模型中的参数数量呈指数级增长，每两年增长 410× 倍，而单个 GPU 内存仅以每 2 年扩展 2× 的速度。Transformer 模型的增长率是通过仅考虑非推荐系统模型（红色圆圈）来计算的，GPU 内存是通过将相应的内存大小除以 6 来绘制的，作为可以使用相应容量训练的最大模型的近似上限。（b） 针对不同的计算机视觉 （CV）、自然语言处理 （NLP） 和语音模型训练 SOTA 模型所需的计算量，以 Peta FLOP 为单位，以及 Transformer 模型的不同规模（750×/2 年）。

自 1998 年 Yann Lecun 在 MNIST 数据上训练著名的 Lenet-5 模型以来，AI 硬件已经发生了变化[22]。我们可以看到，在过去 20 年中，硬件的峰值计算增加了 60,000×，而 DRAM 增加了 100×，互连带宽增加了 30×。

**内存墙问题既涉及有限的容量、内存传输的带宽，也涉及其延迟（这比带宽更难改进[32]）**。这需要不同级别的内存数据传输。例如，计算逻辑和片上存储器之间、计算逻辑和 DRAM 存储器之间或不同插槽上的不同处理器之间的数据传输。在所有这些情况下，数据传输的容量和速度都明显落后于硬件计算能力。

现在，如果我们研究最近人工智能模型的趋势，特别是法学硕士，我们会注意到，在神经缩放定律[14]的激励下，从业者一直在以前所未有的水平扩展训练最新模型所需的数据量、模型大小和计算量。尽管在 2018-2022 年的时间范围内，训练这些最新模型所需的计算/浮点运算 （FLOP） 增加了 750×/2 倍（见图 2），但计算不一定是瓶颈，尤其是对于模型服务而言。

首先，LLM 大小在这段时间范围内以 410×/2 年的速度扩展，超过了单芯片上的可用内存。**人们可能希望我们可以通过将训练/服务横向扩展到多个加速器来使用分布式内存并行性，以避免单个硬件的内存容量和带宽有限**。但是，将工作分布在多个进程上也可能面临内存墙问题：

**神经网络 （NN） 加速器之间移动数据的通信瓶颈，这甚至比片上数据移动更慢、效率更低**。与单一系统内存的情况类似，我们无法克服扩展网络带宽的技术挑战。

**其次，即使模型适合单个芯片，芯片内存储器与寄存器、L2 缓存、全局存储器等之间的传输仍然日益成为瓶颈**。由于 Tensor 内核等专用计算单元的最新进展，大量计算的算术运算可以在几个周期内完成。因此，为了始终使用这些算术单元，需要快速向它们提供大量数据，这就是芯片内存带宽成为瓶颈的地方。

如图 1 所示，在过去 20 年中，服务器硬件 FLOPS 峰值一直以 3.0×/2 年的速度扩展，超过了 DRAM 和互连带宽的增长速度，后者分别仅以每 2 年的 1.6 倍和 1.4 倍的速度扩展。这种差异使得内存而不是计算越来越成为瓶颈，即使对于模型可以安装在单个芯片中的情况也是如此。

接下来，我们对 Transformer 进行了详细的案例研究，通过考虑当今使用的常见模型，它有助于更好地展示 FLOP、内存作 （MOP） 和端到端运行时之间的相互作用。
### 二、案例研究
在本节中，我们首先概述与 Transformer 推理相关的运行时特征和性能瓶颈。我们研究了 Transformer 架构的两种不同变体：编码器架构（例如，BERT [7]），它同时处理所有标记，以及解码器架构（例如，GPT [3， 33]），它自动回归运行，在每次迭代时处理和生成一个标记。


A. 算术强度

衡量性能瓶颈的一种常用方法是计算计算 Transformer 纯编码器和纯解码器模型所需的 FLOP 总数。然而，这个指标单独来看可能非常具有误导性。相反，需要研究所涉及运算的算术强度。算术强度是从内存加载的每个字节可以执行的 FLOP 数。可以通过将 FLOP 总数除以访问的字节总数（也称为 MOP 或内存作）来计算[41]：
![](asserts/Pasted%20image%2020250812212826.png)

为了说明考虑算术强度的重要性，我们研究了 BERT-Base 和 BERT-Large [7]以及 GPT-2 [33]。前两种是编码器模型，涉及矩阵-矩阵运算进行推理，后者是解码器/自回归模型，其推理涉及重复的矩阵-向量乘法。


B. 分析

为了分析商用硬件上 Transformer 工作负载的瓶颈，我们在 Intel Gold 6242 CPU 上分析了 Transformer 推理。图 3 显示了这些模型在不同序列长度下的总 FLOP、MOP、算术强度以及最终延迟。很明显，GPT-2 的延迟明显长于 BERT-Base 或 BERT-Large 的每个序列长度的延迟，即使 BERT-Base 和 GPT-2 具有基本相同的模型配置和端到端 FLOP（如图 3a 所示）。这是由于 GPT 的自回归推理中**固有的矩阵向量运算的较高内存运算和较低的算术强度（见图 3c）**。与算术强度较低的模型相比，具有较高算术强度的模型可以在相同甚至更多的 FLOP 下运行得更快。这清楚地表明了记忆墙是如何可能成为解码器模型（在低批量大小下）和不计算的主要瓶颈。


### 三、PSBW

“没有指数可以永远持续下去”[28]和延迟


即使对于大型超标量公司来说，以 410×/2 年的速度呈指数级扩展也不会长期可行。再加上计算能力和带宽能力之间差距的扩大，很快就会使训练更大的模型变得非常具有挑战性，因为**成本将呈指数级增长**。

为了继续创新并打破内存墙，我们需要重新思考人工智能模型的设计。这里有几个问题。
- 首先，目前**设计人工智能模型的方法大多是临时的，和/或涉及非常简单的缩放规则**。例如，最近的大型 Transformer 模型[3,16,37]大多只是原始 BERT 模型[7]中提出的几乎相同的基本架构的缩放版本。
- **其次，我们需要设计更具数据效率的人工智能模型训练方法。目前的神经网络需要大量的训练数据和数十万次迭代来学习，效率非常低**。有些人可能会注意到，它也与人脑的学习方式不同，人脑的学习方式通常只需要每个概念/类的示例很少。
- 第三，**目前的优化和训练方法需要大量的超参数调优（如学习率、动量等），这往往会导致数百次试错扫描才能找到合适的超参数设置才能成功训练模型**。因此，图 2 （b） 中报告的训练成本只是实际开销的下限，而实际成本通常要高得多。
- 第四，**最先进的模型规模令人望而却步，这使得它们用于推理的部署非常具有挑战性。这不仅限于 GPT-3 等模型**。事实上，部署超标量公司使用的大型推荐系统是一项重大挑战。**最后，硬件加速器的设计主要集中在增加峰值计算上，而对改善内存受限工作负载的关注相对较少**。这使得训练大型模型以及探索替代模型变得困难，例如图神经网络，这些模型通常受带宽限制，无法有效利用当前的加速器。

所有这些问题都是机器学习中的根本问题。在这里，我们简要讨论最近针对最后三个项目的研究（包括我们自己的一些研究）。

#### A. 高效的训练算法


训练 NN 模型的主要挑战之一是需要暴力超参数调整。这包括查找学习率、退火时间表、收敛所需的迭代次数等。这增加了训练 SOTA 模型的（更多）开销。其中许多问题源于用于训练的一阶 SGD 方法。虽然 SGD 变体易于实现，但它们并不健壮
![](asserts/Pasted%20image%2020250812213936.png)
图 3：BERT-Base、BERT-Large 和 GPT-2 模型的分析结果，用于处理/生成批量大小为 1 的不同序列长度。（a） 总推理 FLOP：注意编码器模型如何具有更高的 FLOP;（b） 总推理内存操作 （MOP）：请注意解码器 GPT 模型如何由于其矩阵-向量类型运算与编码器模型中的矩阵-矩阵运算而具有多个数量级的 MOP;（c） 算术强度：请注意 GPT-2 的算术强度小了几个数量级，这使得有效利用给定硬件的计算单元变得非常具有挑战性;（d） 标准化为 BERT-Base 模型以处理 128 输入序列长度的不同模型的端到端延迟：请注意解码器模型的运行时间是如何最慢的，尽管 FLOP 较小。有关更多详细信息，请参阅 [18]。

到超参数调整，并且很难针对新模型进行调整，因为正确的超参数集是未知的。解决这个问题的一种有前途的方法是使用二阶随机优化方法[43]。这些方法通常对超参数调谐更鲁棒，并且可以实现 SOTA[43]。然而，当前方法的内存占用高出 3-4×，这需要解决[43]。一个有前途的工作是 Microsoft 的 Zero 框架，它展示了如何通过删除/分片冗余优化状态变量来训练具有相同内存容量的 8× 大模型 [2， 34]。如果能够解决这些高阶方法的开销，那么它们可以显着降低训练大型模型的总成本。

另一种有前途的方法包括减少内存占用和增加优化算法的数据局部性，但代价是执行更多计算。

在数值线性代数中，一个值得注意的例子是通信规避算法系列[1]。针对 NN 训练的内存进行优化的一个例子是再物质化，其中我们仅在前向传递期间存储/检查激活的子集，而不是保存所有激活。这减少了特征图的内存占用，如图 4 所示。然后可以在需要时重新计算其余的激活[15,19]。尽管这会增加计算量，但可以显着减少内存占用多达 5× [15]，而计算量仅增加 20%。这也可以让从业者在单片机内存上训练大模型，而不是利用分布式训练，后者通常难以设置（在大型超大规模公司之外）也难以调试（对于非专家开发人员而言）。有趣的是，传统趋势表明，新的神经网络模型架构是根据研究人员所能获得的图 4：训练不同 NN 模型所需的内存量。在这里，用于 CV 模型的优化器是 SGD+Momentum，对于 NLP 模型，它是 ADAM。根据可用的 GPU 内存大小发现/设计新模型有一个有趣的趋势。每次增加 GPU 内存容量时，数据科学家都会设计出更新的模型。因此，打破这个所谓的 GPU 内存墙可以进一步允许新的创新。有关检查点的更多详细信息，请参见 [15]。

![](asserts/Pasted%20image%2020250812214411.png)


在单个芯片内，而不是使用复杂的分布式内存方法，请参见图 4 当然，这方面有很多反例来自大型超标量公司，它们有专门的团队来支持研究人员部署大型模型，但当我们考虑整个社区时，这样的例子是有限的。事实上，即使使用最近的 LLM，也经常会花费大量精力来压缩模型，以便它们适合系统，以使更大的研究人员社区可以访问模型。

另一个重要的解决方案是设计对低精度训练具有鲁棒性的优化算法。事实上，人工智能加速器的主要突破之一是使用半精度（FP16）算术，而不是单精度[11,26]。这使得硬件计算能力提高了 10×以上。然而，使用当前的优化方法，要进一步降低精度，从半精度降低到 INT8 一直具有挑战性。最近一个有希望的趋势是混合使用 FP8 和 FP16（甚至最近的 FP4）[27]。该领域的算法创新肯定会使我们能够更有效地利用硬件，并允许芯片的更多区域用于改进内存（通常称为内存间隙惩罚[31]）。

B. 高效部署

部署最近的 SOTA 模型[4,5,16,37]，如 GPT-3[3]或大型推荐系统[29]是相当具有挑战性的，因为它们需要分布式内存部署来进行推理。解决这个问题的一个有前途的解决方案是压缩这些模型进行推理，要么降低精度（即量化），删除（即修剪）它们的冗余参数，要么设计小型语言模型[35]。


第一种方法，量化，可以应用于训练和/或推理步骤。虽然将训练精度降低到远低于 FP16 是非常具有挑战性的，但可以使用超低精度进行推理。使用目前的方法，将推理量化到 INT4 精度相对容易，对精度的影响最小[6,9,17,23,44]。这导致模型占用空间和延迟减少多达 8×[10]。然而，具有亚 INT4 精度的推理更具挑战性，目前是一个非常活跃的研究领域。

第二种方法，修剪，完全删除/修剪模型中的冗余参数。使用目前的方法，可以修剪多达 30%的结构化稀疏神经元，以及高达 80%的非结构化稀疏神经元，对准确性的影响最小[8,13,21]。然而，突破这个限制是非常具有挑战性的，它通常会导致致命的准确性下降。解决这个问题是一个悬而未决的问题。

第三种方法，即小型语言模型，可以开辟全新的领域，并实现人工智能的广泛采用。有趣的是，自 2017 年引入 Transformer 模型以来，用于 LLM 的模型没有改变 [38]。到目前为止，有效的是扩展模型的数据和大小，这导致了这些模型的“紧急能力”[3， 39]。然而，最近对小型语言模型的工作显示出有希望的结果 [35]，关于其能力。如果一个模型可以完全安装在片上，那么这可以带来几个数量级的加速和节能。


C. 重新思考人工智能加速器的设计

同时提高芯片的内存带宽和峰值计算能力存在根本性挑战[32]。然而，牺牲峰值计算以实现更好的计算/带宽权衡是可能的。事实上，CPU 架构已经包含了一个优化良好的缓存层次结构。这就是为什么 CPU 在带宽受限问题上比 GPU 具有更好的性能。这些问题包括大型推荐问题[29]。然而，当今 CPU 的主要挑战是它们的峰值计算能力（即 FLOPS）比 GPU 或 TPU 等 AI 加速器低一个数量级。原因之一是 AI 加速器主要是为了实现最大的峰值计算而设计的。这通常需要删除缓存层次结构等组件，以支持添加更多的计算逻辑。人们可以想象在这两个极端之间有一个替代架构，最好是具有更高效的缓存，重要的是具有更高容量的 DRAM（可能是具有不同带宽的 DRAM 的层次结构）。后者可能非常有助于缓解分布式存储器通信瓶颈[20]。

### 四、结论

在 NLP 中训练最近的 SOTA Transformer 模型的计算成本一直以 750×/2 年的速度扩展，模型参数大小一直以 410×/2 年的速度扩展。相比之下，硬件 FLOPS 的峰值一直在模型参数大小一直以 410×/2 年的速度扩展，速率为 3.0×/2 年，而 DRAM 和互连带宽都越来越落后，扩展率分别为 1.6×/2 年和 1.4×/2 年。从这些数字来看，硬件峰值 FLOPS 在过去 20 年中增加了 60,000×，而 DRAM/互连带宽在同一时期分别仅扩展了 100×/30× 倍。随着这些趋势，**内存——尤其是芯片内/芯片间内存传输——将很快成为服务大型人工智能模型的主要限制因素**。因此，我们需要重新思考人工智能模型的训练、部署和设计，以及我们如何设计人工智能硬件来应对这个日益具有挑战性的内存墙。

我们要感谢 Suresh Krishna 和 Aniruddha Nrusimha 的宝贵反馈。我们感谢 Furiosa 团队的慷慨支持。我们还感谢 Microsoft 通过加速基础模型研究提供的支持，包括 Sean Kuno 的大力支持。此外，我们感谢 Google Cloud、Google TRC 团队，特别是 Jonathan Caton 和 David Patterson 教授的支持。Keutzer 教授的实验室由英特尔公司、英特尔 One-API、英特尔 VLAB 团队、英特尔 One-API 卓越中心、苹果、三星、松下赞助，并通过 BDD 和 BAIR 提供资金。我们感谢 Ellick Chan、Saurabh Tangri、Andres Rodriguez 和 Kittur Ganesh 的大力反馈和支持。Sehoon Kim 感谢韩国高等研究基金会 （KFAS） 的支持。Amir Gholami 得到了三星 SAIT 的资助。Michael W. Mahoney 还想感谢摩根大通教师研究奖以及 DOE、NSF 和 ONR。我们的结论不一定反映我们赞助商的立场或政策，也不应推断出官方认可。





